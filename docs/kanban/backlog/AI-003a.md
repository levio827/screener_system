# AI-003a: Chart Pattern Recognition - Data & Model

## Metadata

| Field | Value |
|-------|-------|
| **ID** | AI-003a |
| **Title** | Chart Pattern Recognition - Dataset & CNN Model |
| **Type** | Feature |
| **Status** | BACKLOG |
| **Priority** | P1 (High) |
| **Estimate** | 12 hours |
| **Sprint** | Sprint 6 |
| **Epic** | AI/Machine Learning Features |
| **Assignee** | TBD |
| **Tags** | #ml #cnn #computer-vision #pattern-recognition |
| **Depends On** | AI-001 |
| **Blocks** | AI-003b |
| **Created** | 2025-11-29 |

## Description

Build comprehensive chart image dataset and train Convolutional Neural Network (CNN) model for automated technical chart pattern recognition. This system generates candlestick chart images from OHLCV data, creates labeled datasets for 10 major patterns, trains a ResNet or EfficientNet-based CNN model, and achieves >70% pattern recognition accuracy. The model enables automated detection of classic technical analysis patterns.

## Problem Statement

- No automated chart pattern detection capability
- Manual pattern identification is time-consuming and subjective
- No labeled dataset for Korean stock market patterns
- Missing infrastructure for chart image generation from OHLCV data
- No CNN model for pattern classification

## Proposed Solution

Build end-to-end chart pattern recognition system:
1. **Chart Image Generator**: Convert OHLCV data to candlestick chart images
2. **Dataset Builder**: Create labeled dataset with 1,000+ samples per pattern
3. **CNN Model Training**: Train ResNet50 or EfficientNetB0 for pattern classification
4. **Model Evaluation**: Achieve >70% accuracy on validation set
5. **Pattern Library**: Support 10 major technical patterns

## Patterns to Detect

### Reversal Patterns
1. **Head and Shoulders** - Bearish reversal, three peaks with middle highest
2. **Inverse Head and Shoulders** - Bullish reversal, three troughs with middle lowest
3. **Double Top** - Bearish reversal, two peaks at similar level
4. **Double Bottom** - Bullish reversal, two troughs at similar level
5. **Rounding Bottom** - Bullish reversal, gradual U-shaped formation

### Continuation Patterns
6. **Triangle** (Ascending/Descending/Symmetrical) - Consolidation before continuation
7. **Flag/Pennant** - Short-term continuation after sharp move
8. **Rectangle** - Horizontal consolidation range
9. **Wedge** (Rising/Falling) - Converging trendlines

### Gap Patterns
10. **Gap Patterns** (Breakaway/Runaway/Exhaustion) - Price discontinuities

## Subtasks

### Phase 1: Chart Image Generation Pipeline (3 hours)
- [ ] Design chart image specifications
  - [ ] Image size: 224x224 pixels (for CNN input)
  - [ ] Candlestick chart with volume overlay
  - [ ] 60-day lookback window per image
  - [ ] No axes, labels, or text (pure pattern)
- [ ] Implement chart generator
  - [ ] Use matplotlib or Pillow for rendering
  - [ ] Candlestick plotting (OHLC bars)
  - [ ] Volume bars at bottom
  - [ ] Normalize price scale per image
- [ ] Create batch generation script
  - [ ] Generate images for all stocks
  - [ ] Sliding window approach (day-by-day)
  - [ ] Save to organized directory structure
- [ ] Add data augmentation
  - [ ] Random scaling (±10%)
  - [ ] Random noise injection
  - [ ] Horizontal flip for symmetric patterns

### Phase 2: Dataset Labeling & Collection (4 hours)
- [ ] Define pattern detection rules
  - [ ] Mathematical criteria for each pattern
  - [ ] Price/volume thresholds
  - [ ] Pattern confirmation signals
- [ ] Implement automatic labeling
  - [ ] Scan historical data for pattern occurrences
  - [ ] Label images with pattern type
  - [ ] Validate with technical analysis rules
- [ ] Manual annotation tool (optional)
  - [ ] Web interface for human labeling
  - [ ] Quality control for auto-labeled data
- [ ] Build balanced dataset
  - [ ] Minimum 1,000 samples per pattern class
  - [ ] 10,000+ total images (10 patterns × 1,000)
  - [ ] 70% train, 15% validation, 15% test split
- [ ] Dataset versioning
  - [ ] Track dataset version (v1, v2, ...)
  - [ ] Store metadata (date range, stocks included)

### Phase 3: CNN Model Architecture (2 hours)
- [ ] Choose base architecture
  - [ ] Option 1: ResNet50 (proven, 25M params)
  - [ ] Option 2: EfficientNetB0 (efficient, 5M params)
  - [ ] Option 3: Custom CNN (lightweight)
- [ ] Implement model
  - [ ] Transfer learning from ImageNet
  - [ ] Replace classification head (10 pattern classes)
  - [ ] Add dropout layers (p=0.5)
  - [ ] Batch normalization
- [ ] Define training configuration
  - [ ] Optimizer: Adam (lr=0.001)
  - [ ] Loss: Categorical cross-entropy
  - [ ] Metrics: Accuracy, F1-score per class
  - [ ] Callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

### Phase 4: Model Training (2 hours)
- [ ] Set up training pipeline
  - [ ] Data loaders with augmentation
  - [ ] GPU utilization (if available)
  - [ ] Mixed precision training
- [ ] Train model
  - [ ] Epochs: 50-100 with early stopping
  - [ ] Batch size: 32
  - [ ] Validation every epoch
- [ ] Hyperparameter tuning
  - [ ] Learning rate: [0.0001, 0.001, 0.01]
  - [ ] Dropout: [0.3, 0.5, 0.7]
  - [ ] Image augmentation strength
- [ ] Log training to MLflow
  - [ ] Loss curves
  - [ ] Accuracy metrics
  - [ ] Confusion matrix
  - [ ] Model artifacts

### Phase 5: Model Evaluation & Validation (1 hour)
- [ ] Evaluate on test set
  - [ ] Overall accuracy > 70%
  - [ ] Per-class F1-score > 0.65
  - [ ] Confusion matrix analysis
- [ ] Error analysis
  - [ ] Identify commonly confused patterns
  - [ ] Analyze failure cases
  - [ ] Visualize misclassifications
- [ ] Generate evaluation report
  - [ ] Classification report
  - [ ] ROC curves for each class
  - [ ] Precision-recall curves
- [ ] Model interpretation
  - [ ] Grad-CAM visualization (what CNN looks at)
  - [ ] Feature map visualization

### Phase 6: Testing & Validation (1 hour)
- [ ] Unit tests for chart generator (5+ tests)
  - [ ] Image dimensions correct
  - [ ] OHLCV data rendered correctly
  - [ ] Augmentation works
- [ ] Unit tests for dataset builder (5+ tests)
  - [ ] Pattern detection rules
  - [ ] Labeling accuracy
  - [ ] Dataset balance
- [ ] Model tests (5+ tests)
  - [ ] Model inference works
  - [ ] Output shape correct
  - [ ] Confidence scores sum to 1.0

## Acceptance Criteria

- [ ] Chart image generation pipeline produces 224x224 images
- [ ] Images contain 60-day candlestick charts with volume
- [ ] Labeled dataset contains 10,000+ images across 10 pattern classes
- [ ] Dataset balanced: 1,000+ samples per pattern
- [ ] Train/validation/test split: 70/15/15
- [ ] CNN model architecture implemented (ResNet50 or EfficientNetB0)
- [ ] Model trained with transfer learning from ImageNet
- [ ] Overall test accuracy > 70%
- [ ] Per-class F1-score > 0.65 for all patterns
- [ ] Training logged to MLflow with metrics and artifacts
- [ ] Confusion matrix shows reasonable pattern discrimination
- [ ] Model evaluation report generated
- [ ] 15+ tests passing (unit + integration)
- [ ] Model serialized and saved (H5 or ONNX format)

## Implementation Details

### Chart Image Generator

```python
# ml/data/chart_generator.py
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.finance import candlestick_ohlc
import numpy as np
from PIL import Image
import io

class ChartImageGenerator:
    """Generate candlestick chart images from OHLCV data"""

    def __init__(self, image_size=(224, 224), lookback_days=60):
        self.image_size = image_size
        self.lookback_days = lookback_days

    def generate_chart(self, ohlcv_data: np.ndarray) -> np.ndarray:
        """
        Generate candlestick chart image

        Args:
            ohlcv_data: Array of shape (N, 5) with OHLC and Volume

        Returns:
            RGB image array of shape (224, 224, 3)
        """
        fig, (ax1, ax2) = plt.subplots(
            2, 1,
            figsize=(self.image_size[0] / 100, self.image_size[1] / 100),
            gridspec_kw={'height_ratios': [3, 1]},
            dpi=100
        )

        # Remove axes, labels, ticks
        for ax in [ax1, ax2]:
            ax.set_xticks([])
            ax.set_yticks([])
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['bottom'].set_visible(False)
            ax.spines['left'].set_visible(False)

        # Plot candlesticks
        self._plot_candlesticks(ax1, ohlcv_data[:, :4])  # OHLC

        # Plot volume
        self._plot_volume(ax2, ohlcv_data[:, 4])  # Volume

        # Convert to image array
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
        buf.seek(0)
        img = Image.open(buf).convert('RGB')
        img = img.resize(self.image_size)

        plt.close(fig)

        return np.array(img)

    def _plot_candlesticks(self, ax, ohlc_data):
        """Plot candlestick chart"""
        for i, (open_, high, low, close) in enumerate(ohlc_data):
            color = 'green' if close >= open_ else 'red'

            # Plot high-low line
            ax.plot([i, i], [low, high], color=color, linewidth=0.5)

            # Plot open-close box
            height = abs(close - open_)
            bottom = min(open_, close)
            rect = mpatches.Rectangle(
                (i - 0.3, bottom), 0.6, height,
                facecolor=color, edgecolor=color
            )
            ax.add_patch(rect)

        ax.set_xlim(-1, len(ohlc_data))
        ax.set_ylim(ohlc_data[:, 2].min() * 0.95, ohlc_data[:, 1].max() * 1.05)

    def _plot_volume(self, ax, volume_data):
        """Plot volume bars"""
        ax.bar(range(len(volume_data)), volume_data, color='blue', alpha=0.5, width=0.8)
        ax.set_xlim(-1, len(volume_data))

    def generate_dataset(
        self,
        stock_codes: list,
        start_date: str,
        end_date: str,
        output_dir: str
    ):
        """
        Generate chart images for multiple stocks

        Args:
            stock_codes: List of stock codes
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            output_dir: Directory to save images
        """
        from app.repositories.price_repository import get_daily_prices

        for stock_code in stock_codes:
            # Get OHLCV data
            prices = get_daily_prices(stock_code, start_date, end_date)

            # Sliding window
            for i in range(self.lookback_days, len(prices)):
                window = prices[i - self.lookback_days:i]

                # Generate image
                img = self.generate_chart(window)

                # Save image
                filename = f"{output_dir}/{stock_code}_{prices[i]['date']}.png"
                Image.fromarray(img).save(filename)

                print(f"Generated: {filename}")
```

### Pattern Detection & Labeling

```python
# ml/data/pattern_detector.py
import numpy as np
from scipy.signal import find_peaks
from typing import Optional

class PatternDetector:
    """Detect technical chart patterns in OHLCV data"""

    def detect_head_and_shoulders(self, prices: np.ndarray) -> bool:
        """
        Detect Head and Shoulders pattern

        Args:
            prices: Close prices array

        Returns:
            True if pattern detected
        """
        # Find peaks
        peaks, _ = find_peaks(prices, distance=5)

        if len(peaks) < 3:
            return False

        # Check for three peaks with middle highest
        left_peak, head, right_peak = peaks[-3:]

        if prices[head] > prices[left_peak] and prices[head] > prices[right_peak]:
            # Check shoulders at similar level (within 2%)
            if abs(prices[left_peak] - prices[right_peak]) / prices[left_peak] < 0.02:
                return True

        return False

    def detect_double_top(self, prices: np.ndarray) -> bool:
        """Detect Double Top pattern"""
        peaks, _ = find_peaks(prices, distance=10)

        if len(peaks) < 2:
            return False

        # Last two peaks
        peak1, peak2 = peaks[-2:]

        # Check peaks at similar level (within 1%)
        if abs(prices[peak1] - prices[peak2]) / prices[peak1] < 0.01:
            # Check trough between peaks
            trough_idx = np.argmin(prices[peak1:peak2]) + peak1
            if prices[trough_idx] < prices[peak1] * 0.95:  # At least 5% drop
                return True

        return False

    def detect_triangle(self, prices: np.ndarray) -> bool:
        """Detect Triangle pattern (Symmetrical)"""
        # Find highs and lows
        highs, _ = find_peaks(prices, distance=5)
        lows, _ = find_peaks(-prices, distance=5)

        if len(highs) < 3 or len(lows) < 3:
            return False

        # Check if highs are descending and lows are ascending (converging)
        high_trend = np.polyfit(highs[-3:], prices[highs[-3:]], 1)
        low_trend = np.polyfit(lows[-3:], prices[lows[-3:]], 1)

        # Highs descending, lows ascending
        if high_trend[0] < 0 and low_trend[0] > 0:
            return True

        return False

    def detect_pattern(self, ohlcv_data: np.ndarray) -> Optional[str]:
        """
        Detect all patterns and return pattern name

        Args:
            ohlcv_data: Array of OHLCV data

        Returns:
            Pattern name or None
        """
        close_prices = ohlcv_data[:, 3]  # Close prices

        if self.detect_head_and_shoulders(close_prices):
            return "head_and_shoulders"
        elif self.detect_double_top(close_prices):
            return "double_top"
        elif self.detect_triangle(close_prices):
            return "triangle"
        # ... more patterns

        return None

    def build_labeled_dataset(
        self,
        stock_codes: list,
        start_date: str,
        end_date: str,
        output_dir: str
    ):
        """
        Generate labeled dataset for all patterns

        Args:
            stock_codes: List of stock codes
            start_date: Start date
            end_date: End date
            output_dir: Output directory
        """
        from ml.data.chart_generator import ChartImageGenerator

        generator = ChartImageGenerator()
        pattern_counts = {}

        for stock_code in stock_codes:
            prices = get_daily_prices(stock_code, start_date, end_date)

            for i in range(60, len(prices)):
                window = prices[i - 60:i]
                ohlcv = np.array([[p['open'], p['high'], p['low'], p['close'], p['volume']] for p in window])

                # Detect pattern
                pattern = self.detect_pattern(ohlcv)

                if pattern:
                    # Generate and save image
                    img = generator.generate_chart(ohlcv)
                    pattern_dir = f"{output_dir}/{pattern}"
                    os.makedirs(pattern_dir, exist_ok=True)

                    filename = f"{pattern_dir}/{stock_code}_{window[-1]['date']}.png"
                    Image.fromarray(img).save(filename)

                    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1

        print("Dataset statistics:")
        for pattern, count in pattern_counts.items():
            print(f"  {pattern}: {count} samples")
```

### CNN Model Architecture

```python
# ml/models/pattern_cnn.py
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import ResNet50, EfficientNetB0

class PatternRecognitionCNN:
    """CNN model for chart pattern recognition"""

    def __init__(self, num_classes=10, architecture='resnet50'):
        self.num_classes = num_classes
        self.architecture = architecture
        self.model = None

    def build_model(self):
        """Build CNN model with transfer learning"""

        # Load pre-trained base model
        if self.architecture == 'resnet50':
            base_model = ResNet50(
                include_top=False,
                weights='imagenet',
                input_shape=(224, 224, 3)
            )
        elif self.architecture == 'efficientnet':
            base_model = EfficientNetB0(
                include_top=False,
                weights='imagenet',
                input_shape=(224, 224, 3)
            )

        # Freeze base model layers
        base_model.trainable = False

        # Add custom classification head
        model = keras.Sequential([
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(512, activation='relu'),
            layers.Dropout(0.5),
            layers.BatchNormalization(),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(self.num_classes, activation='softmax')
        ])

        # Compile model
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', keras.metrics.F1Score(average='macro')]
        )

        self.model = model
        return model

    def train(
        self,
        train_data,
        val_data,
        epochs=50,
        batch_size=32,
        mlflow_tracking=True
    ):
        """Train the model"""

        # Callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            ),
            keras.callbacks.ModelCheckpoint(
                'models/pattern_cnn_best.h5',
                monitor='val_accuracy',
                save_best_only=True
            )
        ]

        # MLflow tracking
        if mlflow_tracking:
            import mlflow
            import mlflow.keras

            with mlflow.start_run(run_name=f"pattern_cnn_{self.architecture}"):
                mlflow.log_params({
                    "architecture": self.architecture,
                    "num_classes": self.num_classes,
                    "epochs": epochs,
                    "batch_size": batch_size,
                    "optimizer": "Adam",
                    "learning_rate": 0.001
                })

                # Train
                history = self.model.fit(
                    train_data,
                    validation_data=val_data,
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=callbacks
                )

                # Log metrics
                mlflow.log_metrics({
                    "train_accuracy": history.history['accuracy'][-1],
                    "val_accuracy": history.history['val_accuracy'][-1],
                    "train_loss": history.history['loss'][-1],
                    "val_loss": history.history['val_loss'][-1]
                })

                # Log model
                mlflow.keras.log_model(self.model, "model")

        return history

    def evaluate(self, test_data):
        """Evaluate model on test set"""
        results = self.model.evaluate(test_data)

        return {
            "test_loss": results[0],
            "test_accuracy": results[1],
            "test_f1_score": results[2]
        }

    def predict(self, image: np.ndarray) -> dict:
        """
        Predict pattern for a single image

        Args:
            image: RGB image array (224, 224, 3)

        Returns:
            {
                "pattern": "head_and_shoulders",
                "confidence": 0.85,
                "probabilities": {...}
            }
        """
        # Preprocess
        img = image / 255.0  # Normalize
        img = np.expand_dims(img, axis=0)  # Add batch dimension

        # Predict
        predictions = self.model.predict(img)[0]

        pattern_names = [
            "head_and_shoulders", "double_top", "double_bottom",
            "triangle", "flag", "cup_and_handle", "wedge",
            "rectangle", "rounding_bottom", "gap"
        ]

        # Get top prediction
        predicted_idx = np.argmax(predictions)
        confidence = predictions[predicted_idx]

        return {
            "pattern": pattern_names[predicted_idx],
            "confidence": float(confidence),
            "probabilities": {name: float(prob) for name, prob in zip(pattern_names, predictions)}
        }
```

### Model Training Script

```python
# ml/training/train_pattern_cnn.py
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from ml.models.pattern_cnn import PatternRecognitionCNN
import mlflow

def main():
    # Data generators with augmentation
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,
        zoom_range=0.1
    )

    val_datagen = ImageDataGenerator(rescale=1./255)

    train_data = train_datagen.flow_from_directory(
        'data/patterns/train',
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical'
    )

    val_data = val_datagen.flow_from_directory(
        'data/patterns/val',
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical'
    )

    # Build and train model
    model = PatternRecognitionCNN(num_classes=10, architecture='resnet50')
    model.build_model()

    print(model.model.summary())

    # Train
    history = model.train(
        train_data=train_data,
        val_data=val_data,
        epochs=50,
        batch_size=32,
        mlflow_tracking=True
    )

    # Evaluate on test set
    test_datagen = ImageDataGenerator(rescale=1./255)
    test_data = test_datagen.flow_from_directory(
        'data/patterns/test',
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical'
    )

    results = model.evaluate(test_data)
    print(f"\nTest Results:")
    print(f"  Accuracy: {results['test_accuracy']:.4f}")
    print(f"  F1 Score: {results['test_f1_score']:.4f}")

    # Save model
    model.model.save('models/pattern_cnn_final.h5')
    print("\nModel saved to models/pattern_cnn_final.h5")

if __name__ == "__main__":
    main()
```

## Testing Strategy

### Unit Tests

```python
# tests/unit/test_chart_generator.py
import pytest
import numpy as np
from ml.data.chart_generator import ChartImageGenerator

class TestChartImageGenerator:

    @pytest.fixture
    def generator(self):
        return ChartImageGenerator(image_size=(224, 224), lookback_days=60)

    @pytest.fixture
    def sample_ohlcv(self):
        """Generate sample OHLCV data"""
        np.random.seed(42)
        return np.random.rand(60, 5) * 100

    def test_generate_chart_dimensions(self, generator, sample_ohlcv):
        """Test chart image has correct dimensions"""
        img = generator.generate_chart(sample_ohlcv)

        assert img.shape == (224, 224, 3)
        assert img.dtype == np.uint8

    def test_generate_chart_values(self, generator, sample_ohlcv):
        """Test chart image has valid pixel values"""
        img = generator.generate_chart(sample_ohlcv)

        assert img.min() >= 0
        assert img.max() <= 255

    def test_generate_different_data(self, generator):
        """Test different data produces different images"""
        data1 = np.random.rand(60, 5) * 100
        data2 = np.random.rand(60, 5) * 100

        img1 = generator.generate_chart(data1)
        img2 = generator.generate_chart(data2)

        assert not np.array_equal(img1, img2)

# tests/unit/test_pattern_detector.py
import pytest
from ml.data.pattern_detector import PatternDetector

class TestPatternDetector:

    @pytest.fixture
    def detector(self):
        return PatternDetector()

    def test_detect_head_and_shoulders(self, detector):
        """Test head and shoulders detection"""
        # Create synthetic H&S pattern
        prices = np.array([
            10, 12, 11,  # Left shoulder
            13, 15, 13,  # Head
            11, 12, 10   # Right shoulder
        ])

        assert detector.detect_head_and_shoulders(prices) == True

    def test_detect_double_top(self, detector):
        """Test double top detection"""
        prices = np.array([
            10, 15, 12, 15, 10  # Two peaks at 15
        ])

        assert detector.detect_double_top(prices) == True

# tests/unit/test_pattern_cnn.py
import pytest
from ml.models.pattern_cnn import PatternRecognitionCNN

class TestPatternRecognitionCNN:

    @pytest.fixture
    def model(self):
        cnn = PatternRecognitionCNN(num_classes=10, architecture='resnet50')
        cnn.build_model()
        return cnn

    def test_model_architecture(self, model):
        """Test model has correct architecture"""
        assert model.model is not None
        assert len(model.model.layers) > 0

    def test_model_output_shape(self, model):
        """Test model output has correct shape"""
        dummy_input = np.random.rand(1, 224, 224, 3).astype(np.float32)
        output = model.model.predict(dummy_input)

        assert output.shape == (1, 10)  # 10 classes

    def test_predict_single_image(self, model):
        """Test prediction on single image"""
        img = np.random.rand(224, 224, 3).astype(np.float32) * 255

        result = model.predict(img)

        assert "pattern" in result
        assert "confidence" in result
        assert 0.0 <= result["confidence"] <= 1.0
```

### Integration Tests

```python
# tests/integration/test_pattern_pipeline.py
import pytest
from ml.data.chart_generator import ChartImageGenerator
from ml.data.pattern_detector import PatternDetector
from ml.models.pattern_cnn import PatternRecognitionCNN

@pytest.mark.integration
def test_full_pipeline():
    """Test full pipeline from data to prediction"""

    # 1. Generate chart image
    generator = ChartImageGenerator()
    ohlcv_data = np.random.rand(60, 5) * 100
    img = generator.generate_chart(ohlcv_data)

    assert img.shape == (224, 224, 3)

    # 2. Detect pattern (ground truth)
    detector = PatternDetector()
    actual_pattern = detector.detect_pattern(ohlcv_data)

    # 3. Predict with CNN
    cnn = PatternRecognitionCNN(num_classes=10)
    cnn.build_model()
    # Load pre-trained weights
    cnn.model.load_weights('models/pattern_cnn_best.h5')

    prediction = cnn.predict(img)

    assert prediction["pattern"] is not None
    assert 0.0 <= prediction["confidence"] <= 1.0
```

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Insufficient labeled data for rare patterns | High | High | Use data augmentation; synthesize patterns; collect more historical data |
| CNN model overfits to training data | Medium | High | Use dropout, batch normalization, and early stopping; cross-validation |
| Pattern definitions subjective/ambiguous | Medium | Medium | Define mathematical criteria; validate with technical analysis experts |
| Chart image quality affects detection | Medium | Medium | Standardize image generation; use high-quality rendering |
| Model fails on noisy/volatile stocks | High | Medium | Include diverse market conditions in training data; ensemble models |
| Computation cost for dataset generation | Low | Medium | Use distributed processing; incremental generation; cache results |
| Pattern detection rules too rigid | Medium | High | Use probabilistic detection; allow tolerance in thresholds |
| Model performance varies by market condition | High | Medium | Train separate models for bull/bear markets; use market regime detection |

## Performance Requirements

- **Chart Image Generation**: < 100ms per image
- **Dataset Generation**: Process 1,000 stocks in < 2 hours
- **Model Training**: Complete in < 4 hours on GPU
- **Model Inference**: < 50ms per image
- **Batch Inference**: Process 100 images in < 3 seconds
- **Model Accuracy**: > 70% overall, > 65% per-class F1-score
- **Memory Usage**: < 4GB for model serving
- **Dataset Size**: 10,000+ labeled images
- **Model Size**: < 200MB for deployment

## Security Considerations

- [ ] Chart images contain no sensitive user data
- [ ] Dataset stored in secure location (S3 with encryption)
- [ ] Model artifacts version-controlled and access-controlled
- [ ] Training data sanitized (no stock codes visible in images)
- [ ] MLflow tracking server authenticated
- [ ] Model served behind authentication layer
- [ ] Rate limiting on pattern detection API
- [ ] Input validation on uploaded images (if user-facing)
- [ ] Prevent adversarial examples (image perturbations)

## Error Handling

### Chart Generation Errors

```python
class ChartImageGenerator:

    def generate_chart(self, ohlcv_data: np.ndarray) -> np.ndarray:
        """Generate chart with error handling"""
        try:
            if len(ohlcv_data) < self.lookback_days:
                raise ValueError(f"Insufficient data: need {self.lookback_days}, got {len(ohlcv_data)}")

            if np.any(np.isnan(ohlcv_data)):
                raise ValueError("OHLCV data contains NaN values")

            # Generate chart
            fig, axes = plt.subplots(...)
            # ... plotting code ...

            return img_array

        except ValueError as e:
            logger.error(f"Validation error: {e}")
            raise

        except Exception as e:
            logger.exception(f"Chart generation failed: {e}")
            # Return placeholder image
            return np.zeros((224, 224, 3), dtype=np.uint8)

        finally:
            plt.close('all')  # Clean up matplotlib resources
```

### Pattern Detection Errors

```python
class PatternDetector:

    def detect_pattern(self, ohlcv_data: np.ndarray) -> Optional[str]:
        """Detect pattern with error handling"""
        try:
            # Validate input
            if len(ohlcv_data) < 20:  # Minimum required
                logger.warning("Insufficient data for pattern detection")
                return None

            # Try each pattern detector
            for pattern_func in [
                self.detect_head_and_shoulders,
                self.detect_double_top,
                # ...
            ]:
                try:
                    if pattern_func(ohlcv_data[:, 3]):  # Close prices
                        return pattern_func.__name__.replace('detect_', '')
                except Exception as e:
                    logger.warning(f"{pattern_func.__name__} failed: {e}")
                    continue

            return None  # No pattern detected

        except Exception as e:
            logger.exception(f"Pattern detection error: {e}")
            return None
```

## Dependencies

- **ML Framework**: TensorFlow/Keras 2.13+
- **Image Processing**: matplotlib, Pillow, OpenCV
- **Data Processing**: NumPy, pandas, scipy
- **Visualization**: matplotlib, seaborn
- **MLflow**: Model tracking and versioning
- **Storage**: S3 or local filesystem for images
- **Depends On**: AI-001 (ML Feature Engineering Pipeline)

## References

- [IMPROVEMENT_TICKETS.md](../../IMPROVEMENT_TICKETS.md) - Epic 1: AI/Machine Learning Features
- [Technical Analysis Patterns](https://www.investopedia.com/trading/chart-patterns/)
- [ResNet Paper](https://arxiv.org/abs/1512.03385)
- [EfficientNet Paper](https://arxiv.org/abs/1905.11946)
- [Transfer Learning Guide](https://www.tensorflow.org/tutorials/images/transfer_learning)

## Progress

**0% - Not started**

## Notes

- Use transfer learning to leverage ImageNet pre-training
- Chart images should be clean (no labels, axes) for better pattern recognition
- Consider using Grad-CAM to visualize what CNN focuses on
- Ensemble multiple models for better robustness
- Pattern detection rules can be improved iteratively based on false positives
- Consider using synthetic pattern generation for rare patterns
- Volume is important for some patterns (e.g., breakout confirmation)
- Model performance may vary by sector and market regime
- Consider using time-series transformers (e.g., TST) as alternative to CNN
- Implement confidence thresholds to reduce false positives
- Add "no_pattern" class for images without clear patterns

## Future Enhancements (Post-MVP)

- [ ] Real-time pattern detection on live charts
- [ ] Multi-timeframe pattern detection (daily, weekly, monthly)
- [ ] Pattern completion prediction (partial pattern detection)
- [ ] Pattern target price prediction
- [ ] Integration with volume analysis
- [ ] Sector-specific pattern models
- [ ] User feedback loop for model improvement
- [ ] Mobile app deployment (TensorFlow Lite)
- [ ] Pattern similarity search
- [ ] Automated pattern backtest (historical performance)
