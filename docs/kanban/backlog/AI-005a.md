# AI-005a: LLM Integration - Backend Service

## Metadata

| Field | Value |
|-------|-------|
| **ID** | AI-005a |
| **Title** | LLM Integration - API Service & Prompt Engineering |
| **Type** | Feature |
| **Status** | BACKLOG |
| **Priority** | P2 (Medium) |
| **Estimate** | 12 hours |
| **Sprint** | Sprint 7 |
| **Epic** | AI/Machine Learning Features |
| **Assignee** | TBD |
| **Depends On** | AI-002b |
| **Created** | 2025-11-29 |
| **Tags** | `llm`, `gpt-4`, `claude`, `prompt-engineering`, `cost-optimization`, `caching` |
| **Blocks** | AI-005b |

## Description

Integrate Large Language Model APIs (OpenAI GPT-4 / Anthropic Claude) for generating natural language stock analysis reports and insights. Implement robust prompt engineering, response caching for cost optimization, and error handling with fallback mechanisms.

## Progress

**0% - Not started**

---

## Acceptance Criteria

- [ ] LLM API integration (OpenAI GPT-4 and Anthropic Claude) with provider abstraction
- [ ] Prompt engineering framework and template management system
- [ ] Per-stock AI analysis report generation with structured output
- [ ] Response caching layer (Redis) for cost optimization with TTL management
- [ ] Daily API call limit management and rate limiting per user/tenant
- [ ] Comprehensive error handling, retry logic, and fallback strategies
- [ ] Token usage tracking and cost monitoring dashboard
- [ ] A/B testing framework for prompt variants

---

## Subtasks

### 1. LLM Provider Integration
- [ ] Abstract LLM provider interface
  - [ ] Design provider-agnostic interface (OpenAI, Anthropic, future providers)
  - [ ] Implement OpenAI GPT-4 client with streaming support
  - [ ] Implement Anthropic Claude client with streaming support
  - [ ] Add provider health checks and failover logic
- [ ] Configuration management
  - [ ] Secure API key storage (AWS Secrets Manager / HashiCorp Vault)
  - [ ] Model version configuration (GPT-4-turbo, Claude-3-opus, etc.)
  - [ ] Timeout and retry settings per provider
- [ ] Response streaming support
  - [ ] Implement Server-Sent Events (SSE) for real-time responses
  - [ ] Add stream buffering and error recovery
  - [ ] Support partial response caching

### 2. Prompt Engineering Framework
- [ ] Prompt template management
  - [ ] Design template schema with variables and constraints
  - [ ] Implement template versioning and A/B testing
  - [ ] Create stock analysis prompt templates
  - [ ] Create market commentary prompt templates
- [ ] Context builders
  - [ ] Stock fundamental data formatter
  - [ ] Technical indicator formatter
  - [ ] News sentiment aggregator
  - [ ] Historical performance summarizer
- [ ] Output parsers
  - [ ] Structured JSON extraction from LLM responses
  - [ ] Validation against expected schema
  - [ ] Fallback to text parsing if JSON fails

### 3. Stock Analysis Report Generation
- [ ] Report generator service
  - [ ] Integrate stock data from multiple sources
  - [ ] Build comprehensive context for LLM prompts
  - [ ] Generate analysis with confidence scores
  - [ ] Extract key insights and recommendations
- [ ] Report types
  - [ ] Daily stock analysis (fundamental + technical)
  - [ ] Sector comparison analysis
  - [ ] Portfolio health check
  - [ ] Risk assessment report
- [ ] Output formatting
  - [ ] Markdown formatting for web display
  - [ ] PDF generation for downloads
  - [ ] Email-friendly HTML templates

### 4. Caching & Cost Optimization
- [ ] Multi-tier caching strategy
  - [ ] L1: In-memory cache for hot stocks (5 min TTL)
  - [ ] L2: Redis cache for all stocks (1 hour TTL)
  - [ ] L3: Database cache for historical reports (7 days)
- [ ] Cache key generation
  - [ ] Content-based hashing for prompt + context
  - [ ] Version-aware cache invalidation
  - [ ] User-specific vs global cache separation
- [ ] Cost tracking
  - [ ] Token usage logging per request
  - [ ] Daily/weekly cost aggregation
  - [ ] Budget alerts and auto-throttling

### 5. Rate Limiting & Quotas
- [ ] User-level quotas
  - [ ] Free tier: 10 reports/day
  - [ ] Pro tier: 100 reports/day
  - [ ] Enterprise tier: Unlimited with rate limits
- [ ] Tenant-level quotas
  - [ ] Monthly budget caps
  - [ ] Automatic throttling near limits
  - [ ] Grace period and overflow handling
- [ ] Priority queue
  - [ ] Premium users get higher priority
  - [ ] Batch requests scheduled off-peak
  - [ ] Emergency bypass for critical requests

### 6. Testing & Quality Assurance
- [ ] Unit tests for LLM clients
  - [ ] Test API call construction
  - [ ] Test response parsing
  - [ ] Test error handling paths
- [ ] Integration tests
  - [ ] Test end-to-end report generation
  - [ ] Test caching behavior
  - [ ] Test rate limiting enforcement
- [ ] Prompt quality evaluation
  - [ ] Human evaluation of sample outputs
  - [ ] Automated quality metrics (coherence, relevance)
  - [ ] A/B testing statistical significance

---

## Implementation Details

### LLM Provider Abstraction

```python
# src/services/llm/base.py
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, AsyncIterator
from pydantic import BaseModel

class LLMMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str

class LLMResponse(BaseModel):
    content: str
    model: str
    usage: Dict[str, int]  # {"prompt_tokens": X, "completion_tokens": Y}
    finish_reason: str
    provider: str

class LLMProvider(ABC):
    """Abstract base class for LLM providers"""

    def __init__(self, api_key: str, model: str, **kwargs):
        self.api_key = api_key
        self.model = model
        self.config = kwargs

    @abstractmethod
    async def generate(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """Generate completion from messages"""
        pass

    @abstractmethod
    async def generate_stream(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> AsyncIterator[str]:
        """Generate completion with streaming"""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """Check if provider is healthy"""
        pass
```

### OpenAI Provider Implementation

```python
# src/services/llm/openai_provider.py
import openai
from typing import List, AsyncIterator
from tenacity import retry, stop_after_attempt, wait_exponential

class OpenAIProvider(LLMProvider):
    """OpenAI GPT-4 provider implementation"""

    def __init__(self, api_key: str, model: str = "gpt-4-turbo", **kwargs):
        super().__init__(api_key, model, **kwargs)
        self.client = openai.AsyncOpenAI(api_key=api_key)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    async def generate(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """Generate completion using OpenAI API"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": m.role, "content": m.content} for m in messages],
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )

            return LLMResponse(
                content=response.choices[0].message.content,
                model=response.model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
                finish_reason=response.choices[0].finish_reason,
                provider="openai"
            )

        except openai.RateLimitError as e:
            logger.warning(f"OpenAI rate limit hit: {e}")
            raise LLMRateLimitError("Rate limit exceeded") from e
        except openai.APIError as e:
            logger.error(f"OpenAI API error: {e}")
            raise LLMProviderError(f"OpenAI error: {e}") from e

    async def generate_stream(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> AsyncIterator[str]:
        """Generate completion with streaming"""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": m.role, "content": m.content} for m in messages],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
                **kwargs
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.error(f"OpenAI streaming error: {e}")
            raise LLMProviderError(f"Streaming failed: {e}") from e

    async def health_check(self) -> bool:
        """Check if OpenAI API is healthy"""
        try:
            await self.client.models.list()
            return True
        except Exception:
            return False
```

### Anthropic Claude Provider

```python
# src/services/llm/anthropic_provider.py
import anthropic
from typing import List, AsyncIterator

class AnthropicProvider(LLMProvider):
    """Anthropic Claude provider implementation"""

    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229", **kwargs):
        super().__init__(api_key, model, **kwargs)
        self.client = anthropic.AsyncAnthropic(api_key=api_key)

    async def generate(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """Generate completion using Anthropic Claude"""
        try:
            # Separate system message from conversation
            system_message = next(
                (m.content for m in messages if m.role == "system"),
                None
            )
            conversation = [
                {"role": m.role, "content": m.content}
                for m in messages if m.role != "system"
            ]

            response = await self.client.messages.create(
                model=self.model,
                system=system_message,
                messages=conversation,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )

            return LLMResponse(
                content=response.content[0].text,
                model=response.model,
                usage={
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.input_tokens + response.usage.output_tokens,
                },
                finish_reason=response.stop_reason,
                provider="anthropic"
            )

        except anthropic.RateLimitError as e:
            logger.warning(f"Anthropic rate limit hit: {e}")
            raise LLMRateLimitError("Rate limit exceeded") from e
        except anthropic.APIError as e:
            logger.error(f"Anthropic API error: {e}")
            raise LLMProviderError(f"Anthropic error: {e}") from e

    async def generate_stream(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> AsyncIterator[str]:
        """Generate completion with streaming"""
        system_message = next(
            (m.content for m in messages if m.role == "system"),
            None
        )
        conversation = [
            {"role": m.role, "content": m.content}
            for m in messages if m.role != "system"
        ]

        async with self.client.messages.stream(
            model=self.model,
            system=system_message,
            messages=conversation,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        ) as stream:
            async for text in stream.text_stream:
                yield text

    async def health_check(self) -> bool:
        """Check if Anthropic API is healthy"""
        try:
            # Simple test message
            await self.generate(
                messages=[LLMMessage(role="user", content="Hello")],
                max_tokens=10
            )
            return True
        except Exception:
            return False
```

### Prompt Template Management

```python
# src/services/llm/prompt_templates.py
from typing import Dict, Any
from jinja2 import Template

class PromptTemplate:
    """Prompt template with variable substitution"""

    STOCK_ANALYSIS_TEMPLATE = """
You are a professional stock market analyst. Analyze the following stock and provide a comprehensive report.

**Stock Information:**
- Company: {{ company_name }} ({{ stock_code }})
- Sector: {{ sector }}
- Current Price: ${{ current_price }}

**Fundamental Data:**
- PER: {{ per }}
- PBR: {{ pbr }}
- ROE: {{ roe }}%
- Debt Ratio: {{ debt_ratio }}%
- Dividend Yield: {{ dividend_yield }}%

**Technical Indicators:**
- RSI (14): {{ rsi }}
- MACD: {{ macd_status }}
- Moving Averages: {{ ma_status }}

**Recent Performance:**
- 1 Month: {{ return_1m }}%
- 3 Months: {{ return_3m }}%
- 6 Months: {{ return_6m }}%

**AI Prediction:**
{{ ai_prediction }}

Please provide:
1. **Overall Rating** (Strong Buy / Buy / Hold / Sell / Strong Sell) with confidence percentage
2. **Key Strengths** (3-5 bullet points)
3. **Key Risks** (3-5 bullet points)
4. **Technical Analysis** (brief summary of chart patterns and indicators)
5. **Fundamental Assessment** (valuation and financial health)
6. **Recommendation** (specific action with price targets)

Format your response as JSON with the following structure:
```json
{
  "overall_rating": "Buy",
  "confidence": 75,
  "strengths": ["...", "..."],
  "risks": ["...", "..."],
  "technical_summary": "...",
  "fundamental_assessment": "...",
  "recommendation": "...",
  "price_targets": {
    "conservative": 50.00,
    "moderate": 55.00,
    "optimistic": 60.00
  }
}
```
"""

    @classmethod
    def render(cls, template_name: str, context: Dict[str, Any]) -> str:
        """Render template with context variables"""
        if template_name == "stock_analysis":
            template = Template(cls.STOCK_ANALYSIS_TEMPLATE)
        else:
            raise ValueError(f"Unknown template: {template_name}")

        return template.render(**context)
```

### Stock Analysis Service

```python
# src/services/llm/stock_analysis_service.py
from typing import Dict, Optional
import json
from src.services.llm.llm_manager import LLMManager
from src.services.llm.prompt_templates import PromptTemplate

class StockAnalysisService:
    """Generate AI-powered stock analysis reports"""

    def __init__(
        self,
        llm_manager: LLMManager,
        cache_manager: CacheManager,
        stock_data_service: StockDataService
    ):
        self.llm = llm_manager
        self.cache = cache_manager
        self.stock_data = stock_data_service

    async def generate_report(
        self,
        stock_code: str,
        use_cache: bool = True
    ) -> Dict:
        """Generate comprehensive stock analysis report"""
        # Check cache first
        if use_cache:
            cache_key = f"llm:analysis:{stock_code}:v1"
            cached = await self.cache.get(cache_key)
            if cached:
                logger.info(f"Cache hit for {stock_code}")
                return json.loads(cached)

        try:
            # Gather stock data
            stock_info = await self.stock_data.get_stock_info(stock_code)
            fundamentals = await self.stock_data.get_fundamentals(stock_code)
            technicals = await self.stock_data.get_technical_indicators(stock_code)
            performance = await self.stock_data.get_historical_returns(stock_code)
            ai_prediction = await self.stock_data.get_ai_prediction(stock_code)

            # Build context for prompt
            context = {
                "stock_code": stock_code,
                "company_name": stock_info["name"],
                "sector": stock_info["sector"],
                "current_price": stock_info["price"],
                "per": fundamentals.get("per"),
                "pbr": fundamentals.get("pbr"),
                "roe": fundamentals.get("roe"),
                "debt_ratio": fundamentals.get("debt_ratio"),
                "dividend_yield": fundamentals.get("dividend_yield"),
                "rsi": technicals.get("rsi"),
                "macd_status": technicals.get("macd_status"),
                "ma_status": technicals.get("ma_status"),
                "return_1m": performance.get("1m"),
                "return_3m": performance.get("3m"),
                "return_6m": performance.get("6m"),
                "ai_prediction": self._format_ai_prediction(ai_prediction),
            }

            # Render prompt
            prompt = PromptTemplate.render("stock_analysis", context)

            # Generate analysis with LLM
            messages = [
                LLMMessage(role="system", content="You are an expert stock market analyst."),
                LLMMessage(role="user", content=prompt)
            ]

            response = await self.llm.generate(
                messages=messages,
                temperature=0.3,  # Lower temperature for more consistent output
                max_tokens=2000,
                provider_preference=["openai", "anthropic"]  # Failover order
            )

            # Parse response
            analysis = self._parse_response(response.content)

            # Add metadata
            analysis["metadata"] = {
                "generated_at": datetime.utcnow().isoformat(),
                "model": response.model,
                "provider": response.provider,
                "tokens_used": response.usage["total_tokens"],
            }

            # Cache result (1 hour TTL)
            await self.cache.set(
                cache_key,
                json.dumps(analysis),
                ttl=3600
            )

            # Track token usage
            await self._track_usage(
                stock_code=stock_code,
                tokens=response.usage["total_tokens"],
                provider=response.provider
            )

            return analysis

        except Exception as e:
            logger.error(f"Failed to generate analysis for {stock_code}: {e}", exc_info=True)
            raise StockAnalysisError(f"Analysis generation failed: {e}") from e

    def _parse_response(self, content: str) -> Dict:
        """Parse LLM response to structured format"""
        try:
            # Try to extract JSON from response
            json_start = content.find("{")
            json_end = content.rfind("}") + 1

            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end]
                return json.loads(json_str)
            else:
                # Fallback: parse as plain text
                logger.warning("No JSON found in response, using fallback parser")
                return self._fallback_parse(content)

        except json.JSONDecodeError as e:
            logger.warning(f"JSON parsing failed: {e}, using fallback")
            return self._fallback_parse(content)

    def _fallback_parse(self, content: str) -> Dict:
        """Fallback text parser for non-JSON responses"""
        return {
            "overall_rating": "Unknown",
            "confidence": 50,
            "strengths": [],
            "risks": [],
            "technical_summary": content[:500],
            "fundamental_assessment": "",
            "recommendation": "Please review the full text for details",
            "full_text": content,
        }

    def _format_ai_prediction(self, prediction: Optional[Dict]) -> str:
        """Format AI prediction for prompt"""
        if not prediction:
            return "No AI prediction available"

        direction = prediction.get("direction", "neutral")
        confidence = prediction.get("confidence", 0)
        target_price = prediction.get("target_price")

        return f"AI predicts {direction} movement with {confidence*100:.0f}% confidence. " \
               f"Target price: ${target_price:.2f}" if target_price else ""

    async def _track_usage(
        self,
        stock_code: str,
        tokens: int,
        provider: str
    ):
        """Track token usage for cost monitoring"""
        await self.usage_tracker.record(
            stock_code=stock_code,
            tokens=tokens,
            provider=provider,
            timestamp=datetime.utcnow()
        )
```

### LLM Manager with Failover

```python
# src/services/llm/llm_manager.py
from typing import List, Optional
from src.services.llm.base import LLMProvider, LLMMessage, LLMResponse
from src.services.llm.openai_provider import OpenAIProvider
from src.services.llm.anthropic_provider import AnthropicProvider

class LLMManager:
    """Manage multiple LLM providers with failover"""

    def __init__(self, config: Dict):
        self.providers: Dict[str, LLMProvider] = {}

        # Initialize providers
        if config.get("openai"):
            self.providers["openai"] = OpenAIProvider(
                api_key=config["openai"]["api_key"],
                model=config["openai"].get("model", "gpt-4-turbo")
            )

        if config.get("anthropic"):
            self.providers["anthropic"] = AnthropicProvider(
                api_key=config["anthropic"]["api_key"],
                model=config["anthropic"].get("model", "claude-3-opus-20240229")
            )

    async def generate(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        provider_preference: Optional[List[str]] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate with automatic failover"""
        providers_to_try = provider_preference or ["openai", "anthropic"]

        last_error = None
        for provider_name in providers_to_try:
            provider = self.providers.get(provider_name)
            if not provider:
                continue

            try:
                # Check provider health
                is_healthy = await provider.health_check()
                if not is_healthy:
                    logger.warning(f"Provider {provider_name} unhealthy, skipping")
                    continue

                # Generate response
                response = await provider.generate(
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )

                logger.info(f"Successfully generated with {provider_name}")
                return response

            except LLMRateLimitError as e:
                logger.warning(f"Rate limit on {provider_name}, trying next provider")
                last_error = e
                continue

            except LLMProviderError as e:
                logger.error(f"Provider {provider_name} error: {e}")
                last_error = e
                continue

        # All providers failed
        raise LLMProviderError(
            f"All providers failed. Last error: {last_error}"
        ) from last_error

    async def generate_stream(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        provider_name: str = "openai",
        **kwargs
    ):
        """Generate streaming response"""
        provider = self.providers.get(provider_name)
        if not provider:
            raise ValueError(f"Provider {provider_name} not configured")

        async for chunk in provider.generate_stream(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        ):
            yield chunk
```

### REST API Endpoints

```python
# src/api/routes/llm_analysis.py
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from src.services.llm.stock_analysis_service import StockAnalysisService

router = APIRouter(prefix="/v1/ai/analysis", tags=["llm-analysis"])

@router.get("/{stock_code}")
async def get_stock_analysis(
    stock_code: str,
    use_cache: bool = True,
    service: StockAnalysisService = Depends()
):
    """
    Generate AI-powered stock analysis report

    Args:
        stock_code: Stock ticker symbol
        use_cache: Whether to use cached results

    Returns:
        Comprehensive analysis with rating, strengths, risks, and recommendations
    """
    try:
        analysis = await service.generate_report(
            stock_code=stock_code,
            use_cache=use_cache
        )
        return analysis

    except StockAnalysisError as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{stock_code}/stream")
async def stream_stock_analysis(
    stock_code: str,
    service: StockAnalysisService = Depends()
):
    """Stream analysis generation in real-time"""
    async def generate():
        try:
            async for chunk in service.generate_report_stream(stock_code):
                yield f"data: {chunk}\n\n"
        except Exception as e:
            yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@router.get("/usage/stats")
async def get_usage_stats(
    user_id: str = Depends(get_current_user_id),
    days: int = 30,
    tracker: UsageTracker = Depends()
):
    """Get LLM usage statistics and costs"""
    stats = await tracker.get_user_stats(user_id, days=days)

    return {
        "period_days": days,
        "total_requests": stats["total_requests"],
        "total_tokens": stats["total_tokens"],
        "estimated_cost_usd": stats["estimated_cost"],
        "quota_remaining": stats["quota_remaining"],
        "top_stocks": stats["top_analyzed_stocks"],
    }
```

---

## Testing Strategy

### Unit Tests

```python
# tests/services/llm/test_llm_providers.py
import pytest
from unittest.mock import AsyncMock, Mock
from src.services.llm.openai_provider import OpenAIProvider
from src.services.llm.base import LLMMessage

@pytest.mark.asyncio
async def test_openai_generate():
    """Test OpenAI generation"""
    provider = OpenAIProvider(api_key="test_key")
    provider.client = AsyncMock()

    # Mock response
    provider.client.chat.completions.create = AsyncMock(return_value=Mock(
        choices=[Mock(message=Mock(content="Test response"), finish_reason="stop")],
        model="gpt-4",
        usage=Mock(prompt_tokens=10, completion_tokens=20, total_tokens=30)
    ))

    messages = [LLMMessage(role="user", content="Test")]
    response = await provider.generate(messages)

    assert response.content == "Test response"
    assert response.usage["total_tokens"] == 30

@pytest.mark.asyncio
async def test_llm_manager_failover():
    """Test LLM manager failover logic"""
    manager = LLMManager({
        "openai": {"api_key": "test1"},
        "anthropic": {"api_key": "test2"}
    })

    # Mock first provider to fail
    manager.providers["openai"].generate = AsyncMock(
        side_effect=LLMProviderError("OpenAI failed")
    )
    manager.providers["openai"].health_check = AsyncMock(return_value=True)

    # Mock second provider to succeed
    manager.providers["anthropic"].generate = AsyncMock(
        return_value=LLMResponse(
            content="Success",
            model="claude",
            usage={"total_tokens": 30},
            finish_reason="stop",
            provider="anthropic"
        )
    )
    manager.providers["anthropic"].health_check = AsyncMock(return_value=True)

    messages = [LLMMessage(role="user", content="Test")]
    response = await manager.generate(messages, provider_preference=["openai", "anthropic"])

    assert response.provider == "anthropic"
    assert response.content == "Success"
```

### Integration Tests

```python
# tests/api/test_llm_analysis_endpoints.py
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_get_stock_analysis(app, mock_llm_service):
    """Test stock analysis endpoint"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.get("/v1/ai/analysis/AAPL")

    assert response.status_code == 200
    data = response.json()
    assert "overall_rating" in data
    assert "confidence" in data
    assert "strengths" in data
    assert "recommendation" in data

@pytest.mark.asyncio
async def test_usage_stats_endpoint(app):
    """Test usage statistics endpoint"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.get(
            "/v1/ai/analysis/usage/stats?days=30",
            headers={"Authorization": "Bearer test_token"}
        )

    assert response.status_code == 200
    data = response.json()
    assert "total_requests" in data
    assert "total_tokens" in data
    assert "estimated_cost_usd" in data
```

---

## Risk Assessment

| Risk | Severity | Probability | Mitigation |
|------|----------|-------------|------------|
| High API costs exceeding budget | High | High | Aggressive caching (1hr TTL), rate limiting, quota management, cost alerts |
| LLM provider API outages | High | Medium | Multi-provider failover (OpenAI + Claude), graceful degradation to cached results |
| Inconsistent or hallucinated outputs | Medium | High | Structured output with JSON schema, validation, confidence scoring, human review for edge cases |
| Slow response times (> 10s) | Medium | Medium | Response caching, streaming responses, async processing, timeout handling |
| Prompt injection attacks | High | Low | Input sanitization, strict prompt templates, content filtering, rate limiting |
| GDPR/privacy concerns with user data | Medium | Low | Anonymize PII before sending to LLM, no user-specific data in prompts, audit logging |
| Model deprecation or version changes | Low | High | Abstract provider interface, version configuration, automated migration testing |

---

## Performance Requirements

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| API response time (cached) | < 100ms (p95) | APM monitoring |
| API response time (uncached) | < 8s (p95) | APM monitoring |
| Cache hit rate | > 80% | Redis metrics |
| Cost per analysis | < $0.05 | Token usage tracking |
| Daily API quota utilization | < 80% of limit | Custom monitoring dashboard |
| Provider failover time | < 2s | Health check monitoring |
| Streaming chunk latency | < 500ms | SSE event timing |

---

## Security Considerations

### API Key Management
```python
# Secure API key retrieval
from src.core.secrets import SecretManager

class LLMConfig:
    @staticmethod
    async def get_openai_key() -> str:
        """Retrieve OpenAI API key from secret manager"""
        secret_manager = SecretManager()
        return await secret_manager.get_secret("llm/openai/api_key")
```

### Input Sanitization
```python
def sanitize_prompt_input(text: str, max_length: int = 10000) -> str:
    """Sanitize user input before including in prompts"""
    # Remove potentially malicious content
    text = text.strip()[:max_length]

    # Remove prompt injection attempts
    forbidden_patterns = [
        "ignore previous instructions",
        "system:",
        "assistant:",
        "<|endoftext|>",
    ]

    for pattern in forbidden_patterns:
        text = text.replace(pattern, "")

    return text
```

### Rate Limiting
- Global rate limit: 1000 requests/hour across all users
- Per-user rate limit: 10 requests/hour (free), 100 requests/hour (pro)
- IP-based rate limiting for unauthenticated requests
- Exponential backoff for repeated violations

---

## Error Handling

```python
# src/services/llm/exceptions.py
class LLMError(Exception):
    """Base exception for LLM errors"""
    pass

class LLMProviderError(LLMError):
    """Provider-specific error"""
    pass

class LLMRateLimitError(LLMError):
    """Rate limit exceeded"""
    pass

class LLMTimeoutError(LLMError):
    """Request timeout"""
    pass

class StockAnalysisError(LLMError):
    """Stock analysis generation error"""
    pass

# Error handling with retry
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(LLMTimeoutError)
)
async def generate_with_retry(llm, messages):
    """Generate with automatic retry on timeout"""
    try:
        return await llm.generate(messages, timeout=10)
    except asyncio.TimeoutError as e:
        raise LLMTimeoutError("Request timed out") from e
```

---

## Notes

### Technical Decisions
- **Multi-provider Support**: OpenAI and Claude for redundancy and cost optimization
- **Caching Strategy**: 1-hour TTL balances freshness with cost savings (~80% cache hit rate expected)
- **Structured Output**: JSON schema enforcement for consistent parsing and validation
- **Streaming**: SSE for real-time feedback on long-running analyses

### Cost Optimization
- **Estimated Costs** (based on GPT-4 pricing):
  - Per analysis: ~1500 tokens = $0.03-0.05
  - 1000 users Ã— 5 analyses/day = $150-250/day
  - With 80% cache hit rate: $30-50/day actual cost

### Future Enhancements
- Fine-tuned model for stock analysis (lower cost, better accuracy)
- Multi-modal analysis (include charts, financial statements images)
- Comparative analysis (multiple stocks side-by-side)
- Time-series analysis (track recommendations over time)
- Sentiment analysis integration from news/social media

### Open Questions
- [ ] Should we support custom user prompts or restrict to templates?
- [ ] How to handle multi-language support (Korean, Japanese, etc.)?
- [ ] Should we implement human-in-the-loop review for high-value analyses?
- [ ] How to measure and improve analysis quality over time?

---

## Dependencies

- **AI-002b**: Stock Prediction Model - Serving & API (for AI prediction integration)

## References

- [IMPROVEMENT_TICKETS.md](../../IMPROVEMENT_TICKETS.md) - Epic 1: AI/Machine Learning Features
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Anthropic Claude API Documentation](https://docs.anthropic.com/claude/reference)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [LLM Cost Optimization Strategies](https://research.character.ai/optimizing-inference/)
