# BT-001b: Backtesting Core - Data & Storage

## Metadata

| Field | Value |
|-------|-------|
| **ID** | BT-001b |
| **Title** | Implement Backtesting Data Access & Storage |
| **Type** | Feature |
| **Status** | BACKLOG |
| **Priority** | P0 (Critical) |
| **Estimate** | 12 hours |
| **Sprint** | Sprint 5 |
| **Epic** | Backtesting Engine |
| **Assignee** | TBD |
| **Depends On** | BT-001a |
| **Created** | 2025-11-29 |
| **Tags** | #backend #database #timescaledb #async #caching |
| **Blocks** | BT-002, BT-003 |

## Description

Implement efficient historical data access layer and results storage system for the backtesting engine. This includes optimized TimescaleDB queries for historical price data, benchmark data loading, persistent storage for backtest results and trade history, async task execution with Celery, and intelligent caching strategies to minimize database load and improve performance.

## Subtasks

### Historical Data Repository
- [ ] Create historical data repository class
  - [ ] TimescaleDB connection management
  - [ ] Async query execution
  - [ ] Connection pooling configuration
  - [ ] Query timeout handling
- [ ] Implement price data queries
  - [ ] Fetch OHLCV data by date range
  - [ ] Fetch data for multiple symbols efficiently
  - [ ] Handle missing data gracefully
  - [ ] Apply survivorship bias correction
- [ ] Optimize query performance
  - [ ] Use TimescaleDB time_bucket for aggregation
  - [ ] Implement batch loading for large date ranges
  - [ ] Create materialized views for common queries
  - [ ] Add appropriate database indexes

### Benchmark Data Management
- [ ] Benchmark data loader
  - [ ] Load KOSPI index data
  - [ ] Load KOSDAQ index data
  - [ ] Support custom benchmark indices
  - [ ] Handle index composition changes
- [ ] Benchmark calculation utilities
  - [ ] Calculate index returns
  - [ ] Align dates with portfolio data
  - [ ] Handle missing benchmark data

### Backtest Results Storage
- [ ] Create database schema for results
  - [ ] `backtest_runs` table (metadata)
  - [ ] `backtest_trades` table (trade history)
  - [ ] `backtest_snapshots` table (portfolio snapshots)
  - [ ] `backtest_metrics` table (performance metrics)
- [ ] Implement results repository
  - [ ] Save backtest run metadata
  - [ ] Bulk insert trade records
  - [ ] Store portfolio snapshots
  - [ ] Store calculated metrics
- [ ] Implement query methods
  - [ ] Get backtest by ID
  - [ ] Get user's backtest history
  - [ ] Get trade history for backtest
  - [ ] Get portfolio snapshots

### Async Task Execution (Celery)
- [ ] Configure Celery for backtesting
  - [ ] Set up Celery worker
  - [ ] Configure Redis as message broker
  - [ ] Define task queues (priority levels)
  - [ ] Configure task time limits
- [ ] Implement backtest task
  - [ ] Create async backtest task
  - [ ] Progress tracking/reporting
  - [ ] Task status updates
  - [ ] Error handling and retries
- [ ] Task monitoring
  - [ ] Task result backend configuration
  - [ ] Task status endpoint
  - [ ] Failed task logging

### Caching Strategy
- [ ] Implement cache layer
  - [ ] Redis cache configuration
  - [ ] Cache key design
  - [ ] TTL configuration by data type
  - [ ] Cache invalidation strategy
- [ ] Cache historical data
  - [ ] Cache frequently accessed price data
  - [ ] Cache benchmark data
  - [ ] Implement LRU eviction
- [ ] Cache backtest results
  - [ ] Cache completed backtest results
  - [ ] Cache intermediate calculations
  - [ ] Warm cache for common queries

### Testing
- [ ] Unit tests for repositories
  - [ ] Test data loading methods
  - [ ] Test query filters
  - [ ] Mock database responses
- [ ] Integration tests
  - [ ] Test with real TimescaleDB
  - [ ] Test data integrity
  - [ ] Test concurrent access
- [ ] Performance tests
  - [ ] Benchmark query performance
  - [ ] Test cache hit rates
  - [ ] Load testing for Celery tasks

## Implementation Details

### Database Schema

```sql
-- Backtest run metadata
CREATE TABLE backtest_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    strategy_id UUID REFERENCES screening_strategies(id),
    strategy_snapshot JSONB NOT NULL,  -- Snapshot of strategy at run time
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    initial_capital DECIMAL(15, 2) NOT NULL,
    rebalancing_period VARCHAR(20) NOT NULL,
    commission_rate DECIMAL(6, 4),
    slippage_rate DECIMAL(6, 4),
    position_sizing VARCHAR(50),
    status VARCHAR(20) DEFAULT 'pending',  -- pending, running, completed, failed
    error_message TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,

    INDEX idx_backtest_runs_user_id (user_id),
    INDEX idx_backtest_runs_status (status),
    INDEX idx_backtest_runs_created_at (created_at DESC)
);

-- Trade history
CREATE TABLE backtest_trades (
    id BIGSERIAL PRIMARY KEY,
    backtest_id UUID NOT NULL REFERENCES backtest_runs(id) ON DELETE CASCADE,
    trade_date DATE NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    action VARCHAR(10) NOT NULL,  -- BUY, SELL
    quantity INTEGER NOT NULL,
    price DECIMAL(12, 2) NOT NULL,
    commission DECIMAL(12, 2) NOT NULL,
    slippage DECIMAL(12, 2) NOT NULL,
    total_cost DECIMAL(15, 2) NOT NULL,

    INDEX idx_backtest_trades_backtest_id (backtest_id),
    INDEX idx_backtest_trades_date (trade_date)
);

-- Portfolio snapshots
CREATE TABLE backtest_snapshots (
    id BIGSERIAL PRIMARY KEY,
    backtest_id UUID NOT NULL REFERENCES backtest_runs(id) ON DELETE CASCADE,
    snapshot_date DATE NOT NULL,
    cash DECIMAL(15, 2) NOT NULL,
    positions JSONB NOT NULL,  -- {symbol: quantity}
    position_values JSONB NOT NULL,  -- {symbol: market_value}
    total_value DECIMAL(15, 2) NOT NULL,

    INDEX idx_backtest_snapshots_backtest_id (backtest_id),
    INDEX idx_backtest_snapshots_date (snapshot_date),
    UNIQUE(backtest_id, snapshot_date)
);

-- SELECT * FROM backtest_snapshots WHERE backtest_id = '...' ORDER BY snapshot_date;
SELECT create_hypertable('backtest_snapshots', 'snapshot_date',
    chunk_time_interval => INTERVAL '1 month',
    if_not_exists => TRUE);

-- Performance metrics
CREATE TABLE backtest_metrics (
    backtest_id UUID PRIMARY KEY REFERENCES backtest_runs(id) ON DELETE CASCADE,
    total_return DECIMAL(10, 4),
    cagr DECIMAL(10, 4),
    volatility DECIMAL(10, 4),
    sharpe_ratio DECIMAL(10, 4),
    sortino_ratio DECIMAL(10, 4),
    max_drawdown DECIMAL(10, 4),
    max_drawdown_duration INTEGER,  -- days
    win_rate DECIMAL(6, 4),
    profit_loss_ratio DECIMAL(10, 4),
    alpha DECIMAL(10, 4),
    beta DECIMAL(10, 4),
    information_ratio DECIMAL(10, 4),
    total_trades INTEGER,
    avg_holding_period DECIMAL(10, 2),
    benchmark_return DECIMAL(10, 4),

    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Historical Data Repository

```python
# backend/app/repositories/backtesting/historical_data_repository.py
from datetime import date
from typing import List, Dict, Optional
import pandas as pd
from sqlalchemy import select, and_
from sqlalchemy.ext.asyncio import AsyncSession
from app.db.models import DailyPrice
from app.core.cache import CacheManager

class HistoricalDataRepository:
    """Repository for accessing historical price data"""

    def __init__(self, db: AsyncSession, cache: CacheManager):
        self.db = db
        self.cache = cache

    async def get_price_data(
        self,
        symbols: List[str],
        start_date: date,
        end_date: date,
        use_cache: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch historical OHLCV data for multiple symbols

        Args:
            symbols: List of stock codes
            start_date: Start date for data
            end_date: End date for data
            use_cache: Whether to use cached data

        Returns:
            Dict mapping symbol to DataFrame with columns:
            [date, open, high, low, close, volume]
        """
        result = {}

        for symbol in symbols:
            # Check cache first
            cache_key = f"price_data:{symbol}:{start_date}:{end_date}"

            if use_cache:
                cached = await self.cache.get(cache_key)
                if cached:
                    result[symbol] = pd.read_json(cached)
                    continue

            # Query database
            query = select(DailyPrice).where(
                and_(
                    DailyPrice.code == symbol,
                    DailyPrice.date >= start_date,
                    DailyPrice.date <= end_date
                )
            ).order_by(DailyPrice.date)

            rows = await self.db.execute(query)
            data = rows.scalars().all()

            if data:
                df = pd.DataFrame([
                    {
                        'date': row.date,
                        'open': float(row.open_price),
                        'high': float(row.high_price),
                        'low': float(row.low_price),
                        'close': float(row.close_price),
                        'volume': row.volume
                    }
                    for row in data
                ])
                df.set_index('date', inplace=True)

                result[symbol] = df

                # Cache for 1 hour
                await self.cache.set(
                    cache_key,
                    df.to_json(),
                    ttl=3600
                )
            else:
                # Return empty DataFrame for missing symbols
                result[symbol] = pd.DataFrame(
                    columns=['date', 'open', 'high', 'low', 'close', 'volume']
                ).set_index('date')

        return result

    async def get_benchmark_data(
        self,
        benchmark: str,  # 'KOSPI' or 'KOSDAQ'
        start_date: date,
        end_date: date
    ) -> pd.DataFrame:
        """Fetch benchmark index data"""
        cache_key = f"benchmark:{benchmark}:{start_date}:{end_date}"

        cached = await self.cache.get(cache_key)
        if cached:
            return pd.read_json(cached)

        # Query index data (assuming index_daily_prices table)
        query = select(IndexDailyPrice).where(
            and_(
                IndexDailyPrice.index_name == benchmark,
                IndexDailyPrice.date >= start_date,
                IndexDailyPrice.date <= end_date
            )
        ).order_by(IndexDailyPrice.date)

        rows = await self.db.execute(query)
        data = rows.scalars().all()

        df = pd.DataFrame([
            {
                'date': row.date,
                'close': float(row.close_value)
            }
            for row in data
        ])
        df.set_index('date', inplace=True)

        # Cache for 1 day
        await self.cache.set(cache_key, df.to_json(), ttl=86400)

        return df

    async def get_market_cap_data(
        self,
        symbols: List[str],
        date: date
    ) -> Dict[str, float]:
        """Get market cap data for position sizing"""
        cache_key = f"market_cap:{date}:{','.join(sorted(symbols))}"

        cached = await self.cache.get(cache_key)
        if cached:
            return eval(cached)

        query = select(
            StockInfo.code,
            StockInfo.market_cap
        ).where(
            and_(
                StockInfo.code.in_(symbols),
                StockInfo.as_of_date == date
            )
        )

        rows = await self.db.execute(query)
        result = {row.code: float(row.market_cap) for row in rows}

        # Cache for 1 day
        await self.cache.set(cache_key, str(result), ttl=86400)

        return result
```

### Backtest Results Repository

```python
# backend/app/repositories/backtesting/backtest_repository.py
from uuid import UUID
from typing import List, Optional
from sqlalchemy import select, desc
from sqlalchemy.ext.asyncio import AsyncSession
from app.db.models.backtest import (
    BacktestRun, BacktestTrade, BacktestSnapshot, BacktestMetrics
)
from app.schemas.backtest import BacktestResultResponse

class BacktestRepository:
    """Repository for storing and retrieving backtest results"""

    def __init__(self, db: AsyncSession):
        self.db = db

    async def create_backtest_run(
        self,
        user_id: UUID,
        strategy_id: Optional[UUID],
        strategy_snapshot: dict,
        config: dict
    ) -> BacktestRun:
        """Create new backtest run record"""
        run = BacktestRun(
            user_id=user_id,
            strategy_id=strategy_id,
            strategy_snapshot=strategy_snapshot,
            start_date=config['start_date'],
            end_date=config['end_date'],
            initial_capital=config['initial_capital'],
            rebalancing_period=config['rebalancing_period'],
            commission_rate=config.get('commission_rate'),
            slippage_rate=config.get('slippage_rate'),
            position_sizing=config.get('position_sizing'),
            status='pending'
        )

        self.db.add(run)
        await self.db.commit()
        await self.db.refresh(run)

        return run

    async def update_run_status(
        self,
        run_id: UUID,
        status: str,
        error_message: Optional[str] = None
    ):
        """Update backtest run status"""
        query = select(BacktestRun).where(BacktestRun.id == run_id)
        result = await self.db.execute(query)
        run = result.scalar_one()

        run.status = status
        if error_message:
            run.error_message = error_message

        if status == 'running' and not run.started_at:
            run.started_at = datetime.now(timezone.utc)
        elif status in ('completed', 'failed'):
            run.completed_at = datetime.now(timezone.utc)

        await self.db.commit()

    async def save_trades(
        self,
        backtest_id: UUID,
        trades: List[dict]
    ):
        """Bulk insert trade records"""
        trade_objects = [
            BacktestTrade(
                backtest_id=backtest_id,
                trade_date=trade['date'],
                symbol=trade['symbol'],
                action=trade['action'],
                quantity=trade['quantity'],
                price=trade['price'],
                commission=trade['commission'],
                slippage=trade['slippage'],
                total_cost=trade['total_cost']
            )
            for trade in trades
        ]

        self.db.add_all(trade_objects)
        await self.db.commit()

    async def save_snapshots(
        self,
        backtest_id: UUID,
        snapshots: List[dict]
    ):
        """Bulk insert portfolio snapshots"""
        snapshot_objects = [
            BacktestSnapshot(
                backtest_id=backtest_id,
                snapshot_date=snap['date'],
                cash=snap['cash'],
                positions=snap['positions'],
                position_values=snap['position_values'],
                total_value=snap['total_value']
            )
            for snap in snapshots
        ]

        self.db.add_all(snapshot_objects)
        await self.db.commit()

    async def save_metrics(
        self,
        backtest_id: UUID,
        metrics: dict
    ):
        """Save performance metrics"""
        metric_obj = BacktestMetrics(
            backtest_id=backtest_id,
            **metrics
        )

        self.db.add(metric_obj)
        await self.db.commit()

    async def get_backtest_by_id(
        self,
        backtest_id: UUID,
        user_id: UUID
    ) -> Optional[BacktestRun]:
        """Get backtest run by ID (with ownership check)"""
        query = select(BacktestRun).where(
            and_(
                BacktestRun.id == backtest_id,
                BacktestRun.user_id == user_id
            )
        )

        result = await self.db.execute(query)
        return result.scalar_one_or_none()

    async def get_user_backtests(
        self,
        user_id: UUID,
        limit: int = 20,
        offset: int = 0
    ) -> List[BacktestRun]:
        """Get user's backtest history"""
        query = select(BacktestRun).where(
            BacktestRun.user_id == user_id
        ).order_by(
            desc(BacktestRun.created_at)
        ).limit(limit).offset(offset)

        result = await self.db.execute(query)
        return result.scalars().all()
```

### Celery Task Implementation

```python
# backend/app/tasks/backtesting.py
from celery import Task
from app.worker import celery_app
from app.services.backtesting.engine import BacktestEngine
from app.repositories.backtesting.backtest_repository import BacktestRepository
from app.db.session import AsyncSessionLocal

class BacktestTask(Task):
    """Base task with progress tracking"""

    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """Handle task failure"""
        backtest_id = kwargs.get('backtest_id')
        # Update status in database
        # Log error details

@celery_app.task(
    bind=True,
    base=BacktestTask,
    time_limit=3600,  # 1 hour max
    soft_time_limit=3300  # 55 minutes soft limit
)
async def run_backtest_task(
    self,
    backtest_id: str,
    user_id: str,
    strategy_config: dict,
    backtest_config: dict
):
    """
    Execute backtest asynchronously

    Args:
        backtest_id: UUID of backtest run
        user_id: UUID of user
        strategy_config: Strategy configuration
        backtest_config: Backtest parameters
    """
    async with AsyncSessionLocal() as db:
        repo = BacktestRepository(db)

        try:
            # Update status to running
            await repo.update_run_status(backtest_id, 'running')

            # Initialize engine
            engine = BacktestEngine(backtest_config)

            # Load historical data
            self.update_state(state='PROGRESS', meta={'step': 'loading_data', 'progress': 10})
            # ... data loading logic

            # Run simulation
            self.update_state(state='PROGRESS', meta={'step': 'simulating', 'progress': 30})
            result = await engine.simulate(...)

            # Calculate metrics
            self.update_state(state='PROGRESS', meta={'step': 'calculating_metrics', 'progress': 70})
            # ... metrics calculation

            # Save results
            self.update_state(state='PROGRESS', meta={'step': 'saving_results', 'progress': 90})
            await repo.save_trades(backtest_id, result.trades)
            await repo.save_snapshots(backtest_id, result.snapshots)
            await repo.save_metrics(backtest_id, result.metrics)

            # Mark as completed
            await repo.update_run_status(backtest_id, 'completed')

            return {'status': 'success', 'backtest_id': backtest_id}

        except Exception as e:
            # Mark as failed
            await repo.update_run_status(
                backtest_id,
                'failed',
                error_message=str(e)
            )
            raise
```

## Acceptance Criteria

- [ ] Historical data queries return complete OHLCV data for requested date range
- [ ] Query performance meets targets:
  - [ ] Single symbol, 1 year: < 500ms
  - [ ] 100 symbols, 1 year: < 3 seconds
  - [ ] 500 symbols, 5 years: < 15 seconds
- [ ] Benchmark data (KOSPI/KOSDAQ) loads correctly
- [ ] Backtest results persist to database successfully
- [ ] All four tables (runs, trades, snapshots, metrics) populated correctly
- [ ] Celery tasks execute asynchronously without blocking API
- [ ] Task status updates work correctly (pending → running → completed/failed)
- [ ] Cache hit rate > 50% for repeated queries
- [ ] Concurrent backtests don't interfere with each other
- [ ] User can only access their own backtest results
- [ ] Database connection pool handles concurrent requests efficiently

## Testing Strategy

### Unit Tests

```python
# tests/repositories/test_historical_data_repository.py
import pytest
from datetime import date
from app.repositories.backtesting.historical_data_repository import (
    HistoricalDataRepository
)

@pytest.mark.asyncio
class TestHistoricalDataRepository:
    async def test_get_price_data_single_symbol(self, db_session, cache_manager):
        """Test fetching price data for single symbol"""
        repo = HistoricalDataRepository(db_session, cache_manager)

        data = await repo.get_price_data(
            symbols=['005930'],
            start_date=date(2024, 1, 1),
            end_date=date(2024, 12, 31)
        )

        assert '005930' in data
        assert len(data['005930']) > 0
        assert all(col in data['005930'].columns
                  for col in ['open', 'high', 'low', 'close', 'volume'])

    async def test_get_price_data_caching(self, db_session, cache_manager):
        """Test that price data is cached"""
        repo = HistoricalDataRepository(db_session, cache_manager)

        # First call - should hit database
        data1 = await repo.get_price_data(
            symbols=['005930'],
            start_date=date(2024, 1, 1),
            end_date=date(2024, 1, 31)
        )

        # Second call - should hit cache
        data2 = await repo.get_price_data(
            symbols=['005930'],
            start_date=date(2024, 1, 1),
            end_date=date(2024, 1, 31)
        )

        assert data1['005930'].equals(data2['005930'])
        # Verify cache was used (can mock cache_manager to track calls)

    async def test_get_benchmark_data(self, db_session, cache_manager):
        """Test benchmark data loading"""
        repo = HistoricalDataRepository(db_session, cache_manager)

        data = await repo.get_benchmark_data(
            benchmark='KOSPI',
            start_date=date(2024, 1, 1),
            end_date=date(2024, 12, 31)
        )

        assert len(data) > 0
        assert 'close' in data.columns
```

### Integration Tests

- [ ] Test full backtest workflow from API to database
  - [ ] Submit backtest request
  - [ ] Verify Celery task created
  - [ ] Wait for completion
  - [ ] Verify all data persisted correctly
- [ ] Test concurrent backtests
  - [ ] Run 10 backtests simultaneously
  - [ ] Verify all complete successfully
  - [ ] Check for data corruption
- [ ] Test with large datasets
  - [ ] 1000 symbols, 10 years
  - [ ] Verify memory usage stays within limits
  - [ ] Verify performance meets requirements

### Performance Tests

- [ ] Benchmark database query performance
- [ ] Measure cache effectiveness (hit rate, latency reduction)
- [ ] Load test Celery workers (max concurrent tasks)
- [ ] Profile memory usage during large backtests

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| TimescaleDB query timeout on large requests | Medium | High | Implement query chunking, add timeouts, optimize indexes |
| Cache stampede on popular backtests | Low | Medium | Implement cache locking, stagger cache expiry |
| Celery worker crashes during long backtests | Medium | High | Configure task retries, implement checkpointing |
| Database connection pool exhaustion | Medium | High | Monitor pool usage, set appropriate pool size, connection timeouts |
| Stale cache data | Low | Medium | Set appropriate TTLs, implement cache invalidation on data updates |
| Disk space exhaustion from trade history | Low | Medium | Implement data retention policies, compress old data |

## Performance Requirements

- **Query Performance**:
  - Single symbol, 1 year: < 500ms
  - 100 symbols, 1 year: < 3 seconds
  - 500 symbols, 5 years: < 15 seconds
  - Benchmark data: < 1 second for any date range

- **Storage Performance**:
  - Bulk insert 10,000 trades: < 2 seconds
  - Bulk insert 1,000 snapshots: < 1 second
  - Retrieve backtest results: < 500ms

- **Cache Performance**:
  - Cache hit rate: > 50% for production workloads
  - Cache read latency: < 10ms (95th percentile)
  - Cache write latency: < 20ms (95th percentile)

- **Celery Performance**:
  - Task queue latency: < 1 second
  - Support 20+ concurrent backtest tasks
  - Task result retrieval: < 100ms

## Security Considerations

- [ ] Implement row-level security for backtest results
  - [ ] Users can only access their own backtests
  - [ ] Admin users can access all backtests for support
- [ ] Sanitize all database queries to prevent SQL injection
- [ ] Rate limit backtest submissions per user
  - [ ] Free tier: 10 backtests per day
  - [ ] Pro tier: 100 backtests per day
- [ ] Encrypt sensitive strategy configurations
- [ ] Audit log all backtest submissions and accesses
- [ ] Implement data retention policies
  - [ ] Delete old backtests after 90 days (Free tier)
  - [ ] Retain Pro tier backtests for 1 year

## Error Handling

### Database Error Handling

```python
async def get_price_data_with_retry(
    self,
    symbols: List[str],
    start_date: date,
    end_date: date,
    max_retries: int = 3
) -> Dict[str, pd.DataFrame]:
    """Fetch price data with retry logic"""
    for attempt in range(max_retries):
        try:
            return await self.get_price_data(symbols, start_date, end_date)
        except asyncpg.QueryCanceledError:
            logger.warning(f"Query timeout, attempt {attempt + 1}/{max_retries}")
            if attempt == max_retries - 1:
                raise HTTPException(
                    status_code=504,
                    detail="Historical data query timed out. Try a smaller date range."
                )
        except asyncpg.ConnectionDoesNotExistError:
            logger.error("Database connection lost, reconnecting...")
            await self._reconnect()
        except Exception as e:
            logger.exception("Unexpected error fetching price data")
            raise HTTPException(
                status_code=500,
                detail="Failed to fetch historical data"
            )
```

### Celery Error Handling

```python
@celery_app.task(
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 60},
    retry_backoff=True
)
async def run_backtest_task(...):
    """Backtest task with automatic retries"""
    try:
        # Task logic
        pass
    except MemoryError:
        # Don't retry memory errors
        logger.error("Backtest exceeded memory limit")
        raise Ignore()
    except TimescaleDBQueryTimeout:
        # Retry with exponential backoff
        raise
```

## Dependencies

- **Depends on**:
  - BT-001a: Backtesting Simulation Engine
  - TimescaleDB with historical price data (2.4M+ rows)
  - Redis for caching and Celery message broker
  - PostgreSQL for backtest results storage

- **External Libraries**:
  - asyncpg: Async PostgreSQL driver
  - pandas: Data manipulation
  - celery: Distributed task queue
  - redis: Caching and message broker

- **Blocks**: BT-002, BT-003

## References

- [IMPROVEMENT_TICKETS.md](../../IMPROVEMENT_TICKETS.md) - Epic 2: Backtesting Engine
- **SRS.md**: Section 3.2 Backtesting Requirements
- **SDS.md**: Section 5.3 Backtesting Architecture, 6.2 Database Schema
- [TimescaleDB Best Practices](https://docs.timescale.com/timescaledb/latest/how-to-guides/query-data/)
- [Celery Best Practices](https://docs.celeryq.dev/en/stable/userguide/tasks.html#task-best-practices)
- [SQLAlchemy Async](https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html)

## Progress

- **0%** - Not started

## Notes

- Use connection pooling with appropriate pool size (10-20 connections)
- Implement query timeouts to prevent long-running queries
- Consider using materialized views for commonly accessed aggregations
- Monitor cache hit rates and adjust TTL based on usage patterns
- Implement backtest result cleanup job to manage storage costs
- Future enhancement: distributed caching with Redis Cluster for scalability
- Future enhancement: read replicas for historical data queries
