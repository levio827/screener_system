# BT-005: Walk-Forward Analysis

## Metadata

| Field | Value |
|-------|-------|
| **ID** | BT-005 |
| **Title** | Implement Walk-Forward Optimization |
| **Type** | Feature |
| **Status** | BACKLOG |
| **Priority** | P2 (Medium) |
| **Estimate** | 12 hours |
| **Sprint** | Sprint 7 |
| **Epic** | Backtesting Engine |
| **Assignee** | TBD |
| **Depends On** | BT-001a, BT-002, BT-003 |
| **Blocks** | - |
| **Created** | 2025-11-29 |
| **Updated** | 2025-11-30 |
| **Tags** | `backend`, `optimization`, `walk-forward`, `parameter-tuning`, `overfitting-detection` |

## Description

Implement walk-forward analysis system for strategy parameter optimization and overfitting prevention. This advanced backtesting technique splits historical data into multiple in-sample (training) and out-of-sample (validation) periods, optimizing strategy parameters on training data and validating on unseen data. Includes rolling window implementation, grid search/genetic algorithm optimization, overfitting detection metrics, and visualization of in-sample vs out-of-sample performance.

## Acceptance Criteria

- [ ] **Rolling Window Implementation**
  - [ ] Configurable window sizes (in-sample, out-of-sample)
  - [ ] Anchored vs rolling window modes
  - [ ] Minimum data requirements validation
  - [ ] Window overlap prevention
  - [ ] Progress tracking for multiple windows
- [ ] **Parameter Optimization**
  - [ ] Grid search optimization
  - [ ] Random search optimization
  - [ ] Genetic algorithm optimization (optional)
  - [ ] Multi-objective optimization (return vs risk)
  - [ ] Parameter bounds definition
  - [ ] Configurable optimization metric (Sharpe, return, etc.)
- [ ] **In-Sample/Out-of-Sample Separation**
  - [ ] Strict data separation (no look-ahead bias)
  - [ ] Parameter freezing between periods
  - [ ] Clear period labeling (IS/OOS)
  - [ ] Reoptimization at each window
- [ ] **Overfitting Detection**
  - [ ] IS vs OOS performance comparison
  - [ ] Degradation ratio calculation
  - [ ] Statistical significance testing
  - [ ] Overfitting warning thresholds
  - [ ] Visual degradation indicators
- [ ] **Results Visualization**
  - [ ] Equity curve with IS/OOS periods marked
  - [ ] Performance metrics by period
  - [ ] Parameter evolution over time
  - [ ] Degradation heatmap
  - [ ] Optimal parameter surface plots (2D/3D)
- [ ] **API Integration**
  - [ ] REST endpoint for walk-forward configuration
  - [ ] Long-running task support (Celery)
  - [ ] Progress updates via WebSocket
  - [ ] Results storage and retrieval

## Subtasks

### 1. Walk-Forward Engine Core
- [ ] Design walk-forward architecture
  - [ ] Define window configuration structure
  - [ ] Design period splitting algorithm
  - [ ] Plan optimization workflow
- [ ] Implement window generator
  - [ ] Calculate window boundaries
  - [ ] Validate window sizes
  - [ ] Generate anchored windows
  - [ ] Generate rolling windows
  - [ ] Handle edge cases (insufficient data)
- [ ] Create period manager
  - [ ] Track current period (IS/OOS)
  - [ ] Prevent data leakage
  - [ ] Manage parameter state
  - [ ] Log period transitions

### 2. Parameter Optimization Engine
- [ ] Implement grid search optimizer
  - [ ] Generate parameter grid
  - [ ] Parallel backtest execution
  - [ ] Rank results by objective
  - [ ] Select optimal parameters
- [ ] Implement random search optimizer
  - [ ] Random parameter sampling
  - [ ] Configurable iteration count
  - [ ] Early stopping criteria
  - [ ] Best result tracking
- [ ] Implement genetic algorithm (optional)
  - [ ] Population initialization
  - [ ] Fitness evaluation
  - [ ] Selection (tournament, roulette)
  - [ ] Crossover and mutation
  - [ ] Generation evolution
- [ ] Create optimization framework
  - [ ] Abstract optimizer interface
  - [ ] Objective function wrapper
  - [ ] Constraint handling
  - [ ] Result aggregation

### 3. Data Management
- [ ] Implement data splitter
  - [ ] Split by date ranges
  - [ ] Validate split integrity
  - [ ] Cache split data
  - [ ] Handle missing data periods
- [ ] Create data validator
  - [ ] Check minimum period length
  - [ ] Verify data continuity
  - [ ] Detect gaps and outliers
  - [ ] Validate symbol coverage
- [ ] Implement data loader
  - [ ] Load only required periods
  - [ ] Efficient memory usage
  - [ ] Parallel data loading
  - [ ] Cache management

### 4. Overfitting Detection
- [ ] Calculate degradation metrics
  - [ ] IS vs OOS return comparison
  - [ ] Sharpe ratio degradation
  - [ ] Win rate consistency
  - [ ] Drawdown analysis
- [ ] Implement statistical tests
  - [ ] T-test for return difference
  - [ ] Chi-square test for distribution
  - [ ] Monte Carlo simulation
  - [ ] Confidence intervals
- [ ] Create warning system
  - [ ] Define overfitting thresholds
  - [ ] Flag suspicious patterns
  - [ ] Generate warnings
  - [ ] Suggest remediation
- [ ] Build robustness score
  - [ ] Combine multiple metrics
  - [ ] Weight by importance
  - [ ] Normalize to 0-100 scale
  - [ ] Interpret score levels

### 5. Results Aggregation
- [ ] Aggregate period results
  - [ ] Combine IS and OOS results
  - [ ] Calculate overall metrics
  - [ ] Track parameter changes
  - [ ] Build equity curve
- [ ] Compute summary statistics
  - [ ] Average IS/OOS performance
  - [ ] Degradation statistics
  - [ ] Parameter stability
  - [ ] Best/worst periods
- [ ] Generate comparison tables
  - [ ] Period-by-period comparison
  - [ ] Metric comparison (IS vs OOS)
  - [ ] Parameter usage frequency
  - [ ] Winner analysis

### 6. API Endpoints
- [ ] Create configuration endpoint
  - [ ] `POST /v1/walk-forward` - Start walk-forward
  - [ ] Validate configuration
  - [ ] Create Celery task
  - [ ] Return task ID
- [ ] Implement status endpoint
  - [ ] `GET /v1/walk-forward/{id}` - Get status
  - [ ] Current window progress
  - [ ] Completed windows count
  - [ ] Estimated completion time
- [ ] Create results endpoint
  - [ ] `GET /v1/walk-forward/{id}/results` - Get results
  - [ ] Aggregated metrics
  - [ ] Period details
  - [ ] Parameter evolution
  - [ ] Overfitting analysis
- [ ] Add export endpoint
  - [ ] `GET /v1/walk-forward/{id}/export` - Export data
  - [ ] Full results JSON
  - [ ] CSV exports
  - [ ] Parameter logs

### 7. Visualization Data Generation
- [ ] Generate equity curve data
  - [ ] Mark IS/OOS periods
  - [ ] Color-code by period type
  - [ ] Add parameter change markers
  - [ ] Include benchmark
- [ ] Create performance charts data
  - [ ] Bar chart: IS vs OOS by period
  - [ ] Line chart: Parameter evolution
  - [ ] Heatmap: Degradation matrix
  - [ ] Scatter: IS vs OOS returns
- [ ] Build parameter surface data
  - [ ] 2D heatmap (2 parameters)
  - [ ] 3D surface (3 parameters)
  - [ ] Contour lines
  - [ ] Optimal point marker

### 8. Performance Optimization
- [ ] Implement parallelization
  - [ ] Parallel window processing
  - [ ] Parallel optimization runs
  - [ ] Thread pool configuration
  - [ ] Resource management
- [ ] Add caching layers
  - [ ] Cache historical data
  - [ ] Cache backtest results
  - [ ] Cache optimization results
  - [ ] Invalidation strategy
- [ ] Optimize database queries
  - [ ] Batch inserts for results
  - [ ] Index optimization
  - [ ] Query result caching
  - [ ] Connection pooling
- [ ] Implement progress checkpointing
  - [ ] Save state after each window
  - [ ] Resume from checkpoint
  - [ ] Handle failures gracefully
  - [ ] Cleanup incomplete runs

### 9. Testing
- [ ] Unit tests
  - [ ] Test window generator
  - [ ] Test optimizers
  - [ ] Test degradation calculations
  - [ ] Test data splitter
- [ ] Integration tests
  - [ ] Full walk-forward workflow
  - [ ] API endpoint testing
  - [ ] Database persistence
  - [ ] Error recovery
- [ ] Performance tests
  - [ ] Large dataset handling
  - [ ] Optimization speed
  - [ ] Memory usage
  - [ ] Concurrent runs
- [ ] Validation tests
  - [ ] Compare with manual calculation
  - [ ] Verify no data leakage
  - [ ] Check parameter isolation
  - [ ] Validate statistical tests

### 10. Documentation
- [ ] Create API documentation
  - [ ] Configuration parameters
  - [ ] Response formats
  - [ ] Example requests
  - [ ] Error codes
- [ ] Write methodology guide
  - [ ] Walk-forward concepts
  - [ ] Optimization techniques
  - [ ] Overfitting detection
  - [ ] Best practices
- [ ] Add code comments
  - [ ] Algorithm explanations
  - [ ] Parameter descriptions
  - [ ] Edge case handling
  - [ ] Performance notes

## Implementation Details

### Walk-Forward Configuration Model

```python
# models/walk_forward.py
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Literal
from datetime import date

class ParameterRange(BaseModel):
    """Parameter optimization range."""
    name: str = Field(..., description="Parameter name")
    min: float = Field(..., description="Minimum value")
    max: float = Field(..., description="Maximum value")
    step: Optional[float] = Field(None, description="Step size for grid search")
    type: Literal["int", "float"] = Field("float", description="Parameter type")

class WalkForwardConfig(BaseModel):
    """Walk-forward analysis configuration."""
    strategy_id: int = Field(..., description="Strategy to optimize")
    start_date: date = Field(..., description="Overall start date")
    end_date: date = Field(..., description="Overall end date")

    # Window configuration
    in_sample_months: int = Field(12, ge=3, le=60, description="In-sample period length")
    out_of_sample_months: int = Field(3, ge=1, le=24, description="Out-of-sample period length")
    window_type: Literal["anchored", "rolling"] = Field("rolling", description="Window advancement type")

    # Optimization configuration
    optimizer: Literal["grid", "random", "genetic"] = Field("grid", description="Optimization algorithm")
    parameters: List[ParameterRange] = Field(..., description="Parameters to optimize")
    objective: Literal["sharpe", "return", "sortino", "calmar"] = Field("sharpe", description="Optimization objective")
    max_iterations: Optional[int] = Field(None, description="Max iterations (random/genetic)")

    # Backtesting parameters
    initial_capital: float = Field(100000, ge=1000, le=10_000_000)
    commission: float = Field(0.001, ge=0, le=0.1)
    slippage: float = Field(0.0005, ge=0, le=0.05)

    @validator("parameters")
    def validate_parameters(cls, v):
        if len(v) == 0:
            raise ValueError("At least one parameter required")
        if len(v) > 5:
            raise ValueError("Maximum 5 parameters supported")
        return v

class WalkForwardWindow(BaseModel):
    """Single walk-forward window."""
    window_id: int
    is_start_date: date
    is_end_date: date
    oos_start_date: date
    oos_end_date: date
    optimal_parameters: Optional[Dict[str, float]] = None
    is_metrics: Optional[Dict[str, float]] = None
    oos_metrics: Optional[Dict[str, float]] = None
    degradation_ratio: Optional[float] = None

class WalkForwardResults(BaseModel):
    """Complete walk-forward results."""
    task_id: str
    config: WalkForwardConfig
    windows: List[WalkForwardWindow]
    overall_metrics: Dict[str, float]
    overfitting_score: float
    robustness_score: float
    warnings: List[str]
```

### Walk-Forward Engine Implementation

```python
# services/walk_forward_engine.py
from datetime import date, timedelta
from typing import List, Dict, Tuple
import numpy as np
from dateutil.relativedelta import relativedelta

from models.walk_forward import WalkForwardConfig, WalkForwardWindow
from services.backtesting_engine import BacktestingEngine
from services.parameter_optimizer import ParameterOptimizer
from services.overfitting_detector import OverfittingDetector

class WalkForwardEngine:
    """Walk-forward analysis engine."""

    def __init__(self, config: WalkForwardConfig):
        self.config = config
        self.windows: List[WalkForwardWindow] = []
        self.optimizer = ParameterOptimizer(config)
        self.overfitting_detector = OverfittingDetector()

    def generate_windows(self) -> List[WalkForwardWindow]:
        """Generate walk-forward windows."""
        windows = []
        window_id = 0

        current_is_start = self.config.start_date

        while True:
            # Calculate in-sample period
            is_end = current_is_start + relativedelta(months=self.config.in_sample_months)

            # Calculate out-of-sample period
            oos_start = is_end
            oos_end = oos_start + relativedelta(months=self.config.out_of_sample_months)

            # Check if we've exceeded the overall end date
            if oos_end > self.config.end_date:
                break

            # Create window
            window = WalkForwardWindow(
                window_id=window_id,
                is_start_date=current_is_start,
                is_end_date=is_end,
                oos_start_date=oos_start,
                oos_end_date=oos_end
            )
            windows.append(window)

            window_id += 1

            # Advance window
            if self.config.window_type == "anchored":
                # Anchored: keep start date, extend end
                current_is_start = self.config.start_date
            else:
                # Rolling: move start forward
                current_is_start = oos_start

        if len(windows) < 2:
            raise ValueError(
                f"Insufficient data for walk-forward analysis. "
                f"Need at least 2 windows, got {len(windows)}. "
                f"Total period: {self.config.start_date} to {self.config.end_date}"
            )

        return windows

    async def run_walk_forward(self, progress_callback=None) -> WalkForwardResults:
        """Execute walk-forward analysis."""
        windows = self.generate_windows()
        total_windows = len(windows)

        for idx, window in enumerate(windows):
            # Update progress
            if progress_callback:
                await progress_callback({
                    "window": idx + 1,
                    "total_windows": total_windows,
                    "progress": int((idx / total_windows) * 100),
                    "phase": "optimization"
                })

            # Optimize on in-sample data
            optimal_params = await self._optimize_window(window, is_period=True)
            window.optimal_parameters = optimal_params

            # Backtest on in-sample with optimal parameters
            is_results = await self._backtest_window(window, optimal_params, is_period=True)
            window.is_metrics = self._extract_metrics(is_results)

            # Backtest on out-of-sample with same parameters
            oos_results = await self._backtest_window(window, optimal_params, is_period=False)
            window.oos_metrics = self._extract_metrics(oos_results)

            # Calculate degradation
            window.degradation_ratio = self._calculate_degradation(
                window.is_metrics,
                window.oos_metrics
            )

        # Aggregate results
        overall_metrics = self._aggregate_metrics(windows)

        # Detect overfitting
        overfitting_score = self.overfitting_detector.calculate_score(windows)
        robustness_score = self.overfitting_detector.calculate_robustness(windows)
        warnings = self.overfitting_detector.generate_warnings(windows, overfitting_score)

        return WalkForwardResults(
            task_id=self.config.task_id,
            config=self.config,
            windows=windows,
            overall_metrics=overall_metrics,
            overfitting_score=overfitting_score,
            robustness_score=robustness_score,
            warnings=warnings
        )

    async def _optimize_window(
        self,
        window: WalkForwardWindow,
        is_period: bool
    ) -> Dict[str, float]:
        """Optimize parameters for a window."""
        # Get data for optimization period
        if is_period:
            start_date = window.is_start_date
            end_date = window.is_end_date
        else:
            start_date = window.oos_start_date
            end_date = window.oos_end_date

        # Run optimization
        optimal_params = await self.optimizer.optimize(
            start_date=start_date,
            end_date=end_date,
            objective=self.config.objective
        )

        return optimal_params

    async def _backtest_window(
        self,
        window: WalkForwardWindow,
        parameters: Dict[str, float],
        is_period: bool
    ) -> Dict:
        """Run backtest on a window with given parameters."""
        engine = BacktestingEngine()

        if is_period:
            start_date = window.is_start_date
            end_date = window.is_end_date
        else:
            start_date = window.oos_start_date
            end_date = window.oos_end_date

        # Configure engine with optimized parameters
        engine.configure(
            initial_capital=self.config.initial_capital,
            commission=self.config.commission,
            slippage=self.config.slippage,
            **parameters
        )

        # Run backtest
        results = await engine.run(
            start_date=start_date,
            end_date=end_date,
            strategy_id=self.config.strategy_id
        )

        return results

    def _extract_metrics(self, backtest_results: Dict) -> Dict[str, float]:
        """Extract key metrics from backtest results."""
        return {
            "total_return": backtest_results["total_return"],
            "cagr": backtest_results["cagr"],
            "sharpe_ratio": backtest_results["sharpe_ratio"],
            "sortino_ratio": backtest_results["sortino_ratio"],
            "max_drawdown": backtest_results["max_drawdown"],
            "win_rate": backtest_results["win_rate"],
            "total_trades": backtest_results["total_trades"]
        }

    def _calculate_degradation(
        self,
        is_metrics: Dict[str, float],
        oos_metrics: Dict[str, float]
    ) -> float:
        """Calculate degradation ratio (OOS / IS)."""
        # Use the optimization objective for degradation
        metric_key = {
            "sharpe": "sharpe_ratio",
            "return": "total_return",
            "sortino": "sortino_ratio",
            "calmar": "calmar_ratio"
        }[self.config.objective]

        is_value = is_metrics.get(metric_key, 0)
        oos_value = oos_metrics.get(metric_key, 0)

        if is_value == 0:
            return 0.0

        return oos_value / is_value

    def _aggregate_metrics(self, windows: List[WalkForwardWindow]) -> Dict[str, float]:
        """Aggregate metrics across all windows."""
        # Calculate averages
        avg_is_return = np.mean([w.is_metrics["total_return"] for w in windows])
        avg_oos_return = np.mean([w.oos_metrics["total_return"] for w in windows])
        avg_is_sharpe = np.mean([w.is_metrics["sharpe_ratio"] for w in windows])
        avg_oos_sharpe = np.mean([w.oos_metrics["sharpe_ratio"] for w in windows])
        avg_degradation = np.mean([w.degradation_ratio for w in windows])

        # Calculate consistency metrics
        sharpe_std = np.std([w.oos_metrics["sharpe_ratio"] for w in windows])
        return_std = np.std([w.oos_metrics["total_return"] for w in windows])

        return {
            "avg_is_return": avg_is_return,
            "avg_oos_return": avg_oos_return,
            "avg_is_sharpe": avg_is_sharpe,
            "avg_oos_sharpe": avg_oos_sharpe,
            "avg_degradation": avg_degradation,
            "sharpe_std": sharpe_std,
            "return_std": return_std,
            "total_windows": len(windows),
            "successful_windows": sum(1 for w in windows if w.degradation_ratio > 0.7)
        }
```

### Parameter Optimizer

```python
# services/parameter_optimizer.py
from typing import Dict, List
import numpy as np
from itertools import product
from concurrent.futures import ThreadPoolExecutor, as_completed

from models.walk_forward import WalkForwardConfig, ParameterRange
from services.backtesting_engine import BacktestingEngine

class ParameterOptimizer:
    """Parameter optimization engine."""

    def __init__(self, config: WalkForwardConfig):
        self.config = config

    async def optimize(
        self,
        start_date,
        end_date,
        objective: str
    ) -> Dict[str, float]:
        """Optimize parameters using configured algorithm."""
        if self.config.optimizer == "grid":
            return await self._grid_search(start_date, end_date, objective)
        elif self.config.optimizer == "random":
            return await self._random_search(start_date, end_date, objective)
        elif self.config.optimizer == "genetic":
            return await self._genetic_algorithm(start_date, end_date, objective)

    async def _grid_search(
        self,
        start_date,
        end_date,
        objective: str
    ) -> Dict[str, float]:
        """Grid search optimization."""
        # Generate parameter grid
        param_grid = self._generate_grid()

        # Evaluate all combinations in parallel
        results = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(
                    self._evaluate_params,
                    params,
                    start_date,
                    end_date,
                    objective
                ): params
                for params in param_grid
            }

            for future in as_completed(futures):
                params = futures[future]
                try:
                    score = future.result()
                    results.append((params, score))
                except Exception as e:
                    print(f"Error evaluating {params}: {e}")

        # Find best parameters
        best_params, best_score = max(results, key=lambda x: x[1])

        return best_params

    def _generate_grid(self) -> List[Dict[str, float]]:
        """Generate parameter grid for grid search."""
        param_lists = []
        param_names = []

        for param_range in self.config.parameters:
            param_names.append(param_range.name)

            if param_range.step:
                # Use provided step
                values = np.arange(
                    param_range.min,
                    param_range.max + param_range.step,
                    param_range.step
                )
            else:
                # Default: 10 evenly spaced values
                values = np.linspace(param_range.min, param_range.max, 10)

            if param_range.type == "int":
                values = np.unique(values.astype(int))

            param_lists.append(values)

        # Generate all combinations
        grid = []
        for combination in product(*param_lists):
            params = dict(zip(param_names, combination))
            grid.append(params)

        return grid

    async def _random_search(
        self,
        start_date,
        end_date,
        objective: str
    ) -> Dict[str, float]:
        """Random search optimization."""
        max_iter = self.config.max_iterations or 100

        best_params = None
        best_score = float('-inf')

        for _ in range(max_iter):
            # Sample random parameters
            params = {}
            for param_range in self.config.parameters:
                if param_range.type == "int":
                    value = np.random.randint(param_range.min, param_range.max + 1)
                else:
                    value = np.random.uniform(param_range.min, param_range.max)
                params[param_range.name] = value

            # Evaluate
            score = await self._evaluate_params(params, start_date, end_date, objective)

            if score > best_score:
                best_score = score
                best_params = params

        return best_params

    async def _evaluate_params(
        self,
        params: Dict[str, float],
        start_date,
        end_date,
        objective: str
    ) -> float:
        """Evaluate a parameter set."""
        engine = BacktestingEngine()
        engine.configure(
            initial_capital=self.config.initial_capital,
            commission=self.config.commission,
            slippage=self.config.slippage,
            **params
        )

        results = await engine.run(
            start_date=start_date,
            end_date=end_date,
            strategy_id=self.config.strategy_id
        )

        # Return objective metric
        metric_map = {
            "sharpe": "sharpe_ratio",
            "return": "total_return",
            "sortino": "sortino_ratio",
            "calmar": "calmar_ratio"
        }

        return results.get(metric_map[objective], 0)
```

### Overfitting Detector

```python
# services/overfitting_detector.py
from typing import List, Dict
import numpy as np
from scipy import stats

from models.walk_forward import WalkForwardWindow

class OverfittingDetector:
    """Overfitting detection and analysis."""

    def calculate_score(self, windows: List[WalkForwardWindow]) -> float:
        """Calculate overfitting score (0-100, higher is more overfitting)."""
        if not windows:
            return 0.0

        # Calculate degradation metrics
        degradations = [w.degradation_ratio for w in windows if w.degradation_ratio is not None]

        if not degradations:
            return 0.0

        avg_degradation = np.mean(degradations)
        degradation_std = np.std(degradations)

        # Percentage of windows with severe degradation (< 0.5)
        severe_degradation_pct = sum(1 for d in degradations if d < 0.5) / len(degradations)

        # Combine metrics
        # Perfect case: avg_degradation = 1.0, std = 0, severe = 0 -> score = 0
        # Worst case: avg_degradation = 0, std = high, severe = 1 -> score = 100

        degradation_component = max(0, (1.0 - avg_degradation) * 50)
        consistency_component = min(degradation_std * 25, 25)
        severe_component = severe_degradation_pct * 25

        score = degradation_component + consistency_component + severe_component

        return min(100, max(0, score))

    def calculate_robustness(self, windows: List[WalkForwardWindow]) -> float:
        """Calculate strategy robustness score (0-100, higher is better)."""
        if not windows:
            return 0.0

        # Metrics for robustness
        oos_sharpes = [w.oos_metrics["sharpe_ratio"] for w in windows]
        oos_returns = [w.oos_metrics["total_return"] for w in windows]
        degradations = [w.degradation_ratio for w in windows]

        # 1. Consistency: low standard deviation in OOS Sharpe
        sharpe_consistency = max(0, 1 - (np.std(oos_sharpes) / max(np.mean(oos_sharpes), 0.01)))

        # 2. Positive OOS performance
        positive_sharpe_pct = sum(1 for s in oos_sharpes if s > 1.0) / len(oos_sharpes)

        # 3. Minimal degradation
        avg_degradation = np.mean(degradations)
        degradation_score = min(1.0, avg_degradation)

        # 4. Profitable periods
        profitable_pct = sum(1 for r in oos_returns if r > 0) / len(oos_returns)

        # Combine (weighted average)
        robustness = (
            sharpe_consistency * 0.25 +
            positive_sharpe_pct * 0.25 +
            degradation_score * 0.30 +
            profitable_pct * 0.20
        ) * 100

        return min(100, max(0, robustness))

    def generate_warnings(
        self,
        windows: List[WalkForwardWindow],
        overfitting_score: float
    ) -> List[str]:
        """Generate overfitting warnings."""
        warnings = []

        # Overall overfitting warning
        if overfitting_score > 70:
            warnings.append(
                "CRITICAL: Severe overfitting detected. Strategy may not perform well in live trading."
            )
        elif overfitting_score > 50:
            warnings.append(
                "WARNING: Moderate overfitting detected. Review parameter stability and OOS performance."
            )

        # Degradation warnings
        degradations = [w.degradation_ratio for w in windows]
        avg_deg = np.mean(degradations)

        if avg_deg < 0.5:
            warnings.append(
                f"WARNING: Average degradation ratio is {avg_deg:.2f}. "
                f"OOS performance is only {avg_deg*100:.0f}% of IS performance."
            )

        # Consistency warnings
        if np.std(degradations) > 0.5:
            warnings.append(
                "WARNING: High variance in degradation across windows. "
                "Strategy performance is inconsistent."
            )

        # Individual window warnings
        failed_windows = [
            i for i, w in enumerate(windows)
            if w.oos_metrics["total_return"] < 0
        ]

        if len(failed_windows) > len(windows) / 2:
            warnings.append(
                f"WARNING: {len(failed_windows)} out of {len(windows)} windows "
                f"had negative OOS returns."
            )

        # Statistical significance warning
        is_returns = [w.is_metrics["total_return"] for w in windows]
        oos_returns = [w.oos_metrics["total_return"] for w in windows]

        t_stat, p_value = stats.ttest_rel(is_returns, oos_returns)

        if p_value < 0.05 and np.mean(is_returns) > np.mean(oos_returns):
            warnings.append(
                f"WARNING: IS and OOS returns are significantly different (p={p_value:.4f}). "
                f"Strategy may be overfit to training data."
            )

        return warnings
```

## Testing Strategy

### Unit Tests

```python
# tests/test_walk_forward_engine.py
import pytest
from datetime import date
from services.walk_forward_engine import WalkForwardEngine
from models.walk_forward import WalkForwardConfig, ParameterRange

def test_window_generation_rolling():
    """Test rolling window generation."""
    config = WalkForwardConfig(
        strategy_id=1,
        start_date=date(2020, 1, 1),
        end_date=date(2023, 12, 31),
        in_sample_months=12,
        out_of_sample_months=3,
        window_type="rolling",
        optimizer="grid",
        parameters=[ParameterRange(name="param1", min=1, max=10)],
        objective="sharpe"
    )

    engine = WalkForwardEngine(config)
    windows = engine.generate_windows()

    # Should have multiple windows
    assert len(windows) >= 2

    # Windows should not overlap
    for i in range(len(windows) - 1):
        assert windows[i].oos_end_date <= windows[i+1].is_start_date

def test_degradation_calculation():
    """Test degradation ratio calculation."""
    engine = WalkForwardEngine(mock_config)

    is_metrics = {"sharpe_ratio": 2.0}
    oos_metrics = {"sharpe_ratio": 1.5}

    degradation = engine._calculate_degradation(is_metrics, oos_metrics)

    assert degradation == 0.75  # 1.5 / 2.0
```

### Integration Tests

```python
# tests/test_walk_forward_integration.py
import pytest

@pytest.mark.integration
async def test_full_walk_forward_workflow():
    """Test complete walk-forward workflow."""
    config = WalkForwardConfig(
        strategy_id=1,
        start_date=date(2020, 1, 1),
        end_date=date(2021, 12, 31),
        in_sample_months=6,
        out_of_sample_months=2,
        window_type="rolling",
        optimizer="grid",
        parameters=[
            ParameterRange(name="lookback", min=20, max=60, step=10, type="int")
        ],
        objective="sharpe"
    )

    engine = WalkForwardEngine(config)
    results = await engine.run_walk_forward()

    # Verify results structure
    assert len(results.windows) >= 2
    assert results.overall_metrics is not None
    assert 0 <= results.overfitting_score <= 100
    assert 0 <= results.robustness_score <= 100
```

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Long execution time (hours) | High | Medium | Implement progress tracking, checkpointing, allow resume from failure |
| Memory overflow with many parameters | Medium | High | Limit parameter combinations, implement streaming, use disk caching |
| Numerical instability in optimization | Low | Medium | Add bounds checking, validate intermediate results, handle edge cases |
| Data leakage between periods | Low | Critical | Strict period validation, automated tests, code review |
| Overfitting to optimization metric | High | High | Multi-objective optimization, validate on multiple metrics, documentation |
| Insufficient data for windows | Medium | Medium | Validate data requirements upfront, clear error messages |
| Concurrent execution conflicts | Low | Medium | Use task locks, separate database transactions, queue management |
| Statistical test false positives | Medium | Low | Use multiple tests, adjust significance levels, provide context |

## Performance Requirements

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| Single window optimization | < 5 min for grid search (3 params, 10 values each) | Task duration logging |
| Full walk-forward (4 windows) | < 30 min | End-to-end timing |
| Memory usage | < 2GB per worker | Process monitoring |
| Database query time | < 100ms for result storage | Query profiling |
| Parameter grid generation | < 1s for 5 parameters | Function timing |
| Overfitting score calculation | < 1s | Function timing |

## Security Considerations

- Validate all parameter ranges (prevent resource exhaustion)
- Limit concurrent walk-forward runs per user
- Implement timeout for long-running optimizations
- Sanitize strategy code before execution
- Prevent access to other users' walk-forward results
- Rate limit API endpoints

## Error Handling

```python
# Comprehensive error handling
try:
    windows = engine.generate_windows()
except ValueError as e:
    # Insufficient data
    return {
        "error": "InsufficientData",
        "message": str(e),
        "suggestion": "Reduce window sizes or extend date range"
    }

try:
    results = await engine.run_walk_forward()
except OptimizationTimeout:
    # Optimization took too long
    return {
        "error": "OptimizationTimeout",
        "message": "Parameter optimization exceeded time limit",
        "suggestion": "Reduce parameter range or use random search"
    }
except Exception as e:
    # Unexpected error - save checkpoint
    await engine.save_checkpoint()
    raise
```

## Progress

**0% - Not started**

## Notes

### Implementation Priority

1. **Phase 1**: Window generation and basic walk-forward flow
2. **Phase 2**: Grid search optimization
3. **Phase 3**: Overfitting detection metrics
4. **Phase 4**: Advanced optimizers (random, genetic)

### Dependencies

- **BT-001a**: Requires backtesting simulation engine
- **BT-002**: Requires performance metrics calculator
- **BT-003**: Requires API infrastructure for long-running tasks

### Technical Decisions

1. **Why separate IS/OOS strictly?**
   - Prevent look-ahead bias
   - Realistic performance estimation
   - Industry best practice

2. **Why support multiple optimizers?**
   - Grid search: exhaustive but slow
   - Random search: fast exploration
   - Genetic algorithm: handle complex parameter spaces

3. **Why calculate multiple degradation metrics?**
   - Single metric can be misleading
   - Comprehensive overfitting assessment
   - Better decision support

### Walk-Forward Best Practices

- **Window sizing**: IS period should be 2-4x longer than OOS period
- **Minimum data**: Need at least 2-3 years of data for meaningful analysis
- **Parameter count**: Limit to 3-5 parameters to avoid combinatorial explosion
- **Objective selection**: Use risk-adjusted metrics (Sharpe, Sortino) over raw returns
- **Validation**: Always check degradation ratio and consistency across windows

### Future Enhancements

- **Monte Carlo simulation**: Add robustness testing with randomized data
- **Ensemble optimization**: Combine multiple parameter sets
- **Adaptive windows**: Automatically adjust window sizes based on market regime
- **Parameter correlation analysis**: Detect parameter redundancy
- **Live monitoring**: Track walk-forward parameters in production
- **Cloud scaling**: Distribute optimization across multiple machines
