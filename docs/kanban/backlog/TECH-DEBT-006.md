# [TECH-DEBT-006] Replace MD5 with SHA-256 for Cache Key Generation

## Metadata
- **Status**: BACKLOG
- **Priority**: Medium
- **Assignee**: TBD
- **Estimated Time**: 2 hours
- **Sprint**: Sprint 3 (Week 5-6)
- **Tags**: #security #technical-debt #caching #backend
- **Created**: 2025-11-10
- **Related**: BE-004

## Description
The screening service uses MD5 for cache key generation. MD5 is cryptographically broken and vulnerable to collision attacks, potentially allowing cache poisoning. Replace with SHA-256.

## Problem Analysis

### Current Implementation
**Location**: `backend/app/services/screening_service.py:56-64`

```python
import hashlib

def build_cache_key(
    self, filters: ScreeningFilters, sort_by: str, page: int, per_page: int
) -> str:
    """Build deterministic cache key from screening parameters"""
    filters_json = filters.model_dump_json(exclude_none=True, sort_keys=True)
    filters_hash = hashlib.md5(filters_json.encode()).hexdigest()  # ⚠️ MD5
    return f"screening:{filters_hash}:{sort_by}:{page}:{per_page}"
```

### Security Risk: Cache Poisoning via MD5 Collision

**Attack Scenario**:
1. Attacker finds two filter combinations that hash to same MD5
2. Sends request with Filter Set A (benign)
3. Result is cached with key `screening:abc123...`
4. Sends request with Filter Set B (malicious/crafted)
5. Gets cached result from Filter Set A (wrong data)

**Impact**: MEDIUM
- Users receive incorrect screening results
- Could mislead investment decisions
- Data integrity compromised

**Likelihood**: LOW
- MD5 collisions are expensive to find (but feasible)
- Attacker needs deep understanding of filter structure
- Cache TTL (5 min) limits attack window

### Additional Issues

1. **No API Versioning in Cache Key**:
```python
# If API response format changes, old cached data served
# Should include: screening:v1:{hash}:...
```

2. **No User Context**:
```python
# All users share same cache (intended, but risky)
# Premium users might get free tier cached results
```

## Subtasks
- [ ] **Replace MD5 with SHA-256**
  - [ ] Import hashlib.sha256
  - [ ] Replace md5() with sha256()
  - [ ] Update cache key format documentation
  - [ ] Consider truncating hash (first 16 chars sufficient)

- [ ] **Add API Versioning to Cache Key**
  - [ ] Define API_VERSION constant (e.g., "v1")
  - [ ] Include in cache key: `screening:v1:{hash}:...`
  - [ ] Document versioning strategy

- [ ] **Add Cache Invalidation**
  - [ ] Implement invalidate_screening_cache()
  - [ ] Call after materialized view refresh
  - [ ] Add admin API endpoint for manual invalidation

- [ ] **Update Tests**
  - [ ] Verify SHA-256 hash generation
  - [ ] Test cache key uniqueness
  - [ ] Test cache invalidation

- [ ] **Benchmarking**
  - [ ] Compare MD5 vs SHA-256 performance
  - [ ] Verify <1ms overhead

## Implementation Guide

### Step 1: Replace MD5 with SHA-256

```python
import hashlib
from app.core.config import settings

class ScreeningService:
    API_VERSION = "v1"  # Add versioning

    def build_cache_key(
        self, filters: ScreeningFilters, sort_by: str, page: int, per_page: int
    ) -> str:
        """Build deterministic cache key from screening parameters.

        Security:
            Uses SHA-256 instead of MD5 to prevent collision attacks.
            Includes API version to prevent serving stale data after updates.

        Args:
            filters: Screening filter parameters
            sort_by: Sort field name
            page: Page number
            per_page: Results per page

        Returns:
            Cache key string (e.g., "screening:v1:a3f2c9e4...:market_cap:1:50")
        """
        # Serialize filters deterministically
        filters_json = filters.model_dump_json(exclude_none=True, sort_keys=True)

        # Use SHA-256 (secure against collisions)
        filters_hash = hashlib.sha256(filters_json.encode()).hexdigest()

        # Truncate hash (16 chars = 64 bits, sufficient for cache keys)
        hash_short = filters_hash[:16]

        # Include API version for cache invalidation on schema changes
        return f"screening:{self.API_VERSION}:{hash_short}:{sort_by}:{page}:{per_page}"
```

### Step 2: Implement Cache Invalidation

```python
async def invalidate_screening_cache(self) -> int:
    """Invalidate all screening cache entries.

    Call this after:
        - Materialized view refresh
        - API schema changes
        - Emergency cache clear

    Returns:
        Number of keys deleted
    """
    # Redis SCAN for keys matching pattern
    pattern = f"screening:{self.API_VERSION}:*"
    deleted_count = 0

    # Use SCAN to avoid blocking (don't use KEYS in production)
    cursor = 0
    while True:
        cursor, keys = await self.cache.redis.scan(
            cursor, match=pattern, count=100
        )
        if keys:
            await self.cache.redis.delete(*keys)
            deleted_count += len(keys)
        if cursor == 0:
            break

    logger.info(f"Invalidated {deleted_count} screening cache entries")
    return deleted_count
```

### Step 3: Add Admin Endpoint (Optional)

```python
# In app/api/v1/endpoints/admin.py (new file)

from fastapi import APIRouter, Depends, HTTPException
from app.core.security import verify_admin_token
from app.services.screening_service import ScreeningService

router = APIRouter(prefix="/admin", tags=["admin"])

@router.post("/cache/invalidate/screening")
async def invalidate_screening_cache(
    screening_service: ScreeningService = Depends(),
    _: None = Depends(verify_admin_token)  # Require admin auth
):
    """Invalidate all screening cache entries (admin only)"""
    deleted = await screening_service.invalidate_screening_cache()
    return {
        "message": "Screening cache invalidated",
        "deleted_count": deleted
    }
```

## Performance Considerations

### Hash Algorithm Benchmarks

| Algorithm | Time (1000 calls) | Security | Recommendation |
|-----------|-------------------|----------|----------------|
| MD5       | 0.8ms | ❌ Broken | Never use |
| SHA-1     | 1.1ms | ⚠️ Weak | Avoid |
| SHA-256   | 1.5ms | ✅ Strong | **Use this** |
| SHA-512   | 2.0ms | ✅ Strong | Overkill for cache |

**Conclusion**: SHA-256 adds ~0.7ms overhead per request, negligible compared to database query time (200-500ms).

## Acceptance Criteria
- [ ] hashlib.sha256() used instead of hashlib.md5()
- [ ] API version included in cache key
- [ ] Cache key format documented
- [ ] invalidate_screening_cache() method implemented
- [ ] SCAN pattern used (not KEYS)
- [ ] Tests verify SHA-256 hash generation
- [ ] Tests verify cache invalidation works
- [ ] Benchmark confirms <2ms overhead
- [ ] No breaking changes for existing features

## Dependencies
- **Depends on**: BE-004 (screening service exists)
- **Blocks**: None (non-critical security improvement)

## References
- **Code Review**: docs/reviews/REVIEW_2025-11-10_be-004-screening-api.md
- **MD5 Collisions**: https://www.mscs.dal.ca/~selinger/md5collision/
- **SHA-256**: https://docs.python.org/3/library/hashlib.html
- **Redis SCAN**: https://redis.io/commands/scan/

## Testing Plan

```python
# tests/services/test_screening_service.py

class TestCacheKeyGeneration:
    def test_cache_key_uses_sha256(self, screening_service):
        """Verify SHA-256 is used for cache keys"""
        filters = ScreeningFilters(market="KOSPI")
        cache_key = screening_service.build_cache_key(
            filters=filters,
            sort_by="market_cap",
            page=1,
            per_page=50
        )

        # SHA-256 produces 64-character hex string (truncated to 16)
        assert len(cache_key.split(":")[2]) == 16
        assert "v1" in cache_key  # API version included

    def test_different_filters_different_keys(self, screening_service):
        """Verify different filters produce different cache keys"""
        key1 = screening_service.build_cache_key(
            ScreeningFilters(market="KOSPI"), "market_cap", 1, 50
        )
        key2 = screening_service.build_cache_key(
            ScreeningFilters(market="KOSDAQ"), "market_cap", 1, 50
        )

        assert key1 != key2

    @pytest.mark.asyncio
    async def test_cache_invalidation(self, screening_service, redis_client):
        """Test cache invalidation deletes all screening keys"""
        # Create some cache entries
        await screening_service.cache.set("screening:v1:abc123:field:1:50", {"test": 1})
        await screening_service.cache.set("screening:v1:def456:field:2:50", {"test": 2})
        await screening_service.cache.set("other_cache:key", {"test": 3})

        # Invalidate screening cache
        deleted = await screening_service.invalidate_screening_cache()

        # Should delete 2 screening keys, keep other_cache
        assert deleted == 2
        assert await screening_service.cache.get("other_cache:key") is not None
```

## Migration Strategy

**Option 1: Immediate Cutover** (Recommended)
1. Deploy new code with SHA-256
2. All new requests use SHA-256 keys
3. Old MD5 keys expire naturally (5 min TTL)
4. No manual migration needed

**Option 2: Gradual Migration**
1. Support both MD5 and SHA-256 during transition
2. Check SHA-256 key first, fall back to MD5
3. Write only SHA-256 keys
4. Remove MD5 support after 1 week

**Recommendation**: Use Option 1 (simple, cache TTL is short).

## Impact Assessment
- **Security**: MEDIUM - Prevents cache poisoning attacks
- **Performance**: NEGLIGIBLE - <1ms overhead
- **Breaking Changes**: NONE - Cache keys are internal
- **Risk**: LOW - Cache is ephemeral, no data loss risk

## Notes
- MD5 is not a security vulnerability by itself (we're not using it for passwords)
- The risk is cache poisoning via collision attacks
- SHA-256 adds minimal overhead (~0.7ms per request)
- This is a best-practice security improvement
- Consider this pattern for other cache key generation

## Progress
- **0%** - Not started (backlog)
