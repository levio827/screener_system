# AI-001: ML Feature Engineering Pipeline

## Metadata

| Field | Value |
|-------|-------|
| **ID** | AI-001 |
| **Title** | Implement ML Feature Engineering Pipeline |
| **Type** | Feature |
| **Status** | DONE |
| **Priority** | P0 (Critical) |
| **Estimate** | 16 hours |
| **Actual** | 18 hours |
| **Sprint** | Sprint 5 |
| **Epic** | AI/Machine Learning Features |
| **Assignee** | TBD |
| **Blocks** | AI-002a, AI-002b, AI-003a, AI-003b |
| **Created** | 2025-11-29 |
| **Completed** | 2025-11-29 |
| **Tags** | #ai #ml #feature-engineering #pipeline #data-science |

## Description

Build a pipeline that transforms 200+ indicator data into features suitable for ML model training. This pipeline includes an end-to-end solution covering missing value handling, feature normalization, time series feature generation, and Feature Store implementation. This is the core infrastructure that forms the foundation for all subsequent AI/ML tasks.

## Subtasks

### Feature Extraction
- [x] Analyze `calculated_indicators` table schema
  - [x] Document 200+ indicator metadata
  - [x] Verify data types and ranges per indicator
  - [x] Analyze missing rates (by indicator, by period)
- [x] Implement feature extraction logic
  - [x] Basic price features (OHLCV)
  - [x] Technical indicator features (RSI, MACD, Bollinger Bands, etc.)
  - [x] Fundamental features (PER, PBR, ROE, etc.)
  - [x] Volume-related features

### Missing Value Handling
- [x] Analyze and establish missing value strategy
  - [x] Analyze missing patterns (MCAR, MAR, MNAR)
  - [x] Determine handling strategy per indicator
- [x] Implement missing value handling
  - [x] Forward fill (time series continuity)
  - [x] Linear interpolation
  - [x] Mean/Median imputation (cross-sectional)
  - [x] Feature exclusion logic when missing rate exceeds threshold

### Feature Normalization
- [x] Implement normalization strategies
  - [x] StandardScaler (Z-score normalization)
  - [x] MinMaxScaler (0-1 range)
  - [x] RobustScaler (outlier robust)
- [x] Per-feature scaler selection logic
  - [x] Price-related: MinMaxScaler
  - [x] Ratio indicators: StandardScaler
  - [x] Volume: RobustScaler

### Time Series Features
- [x] Generate lag features
  - [x] 1-day, 5-day, 20-day, 60-day lags
  - [x] Percentage change lags (pct_change)
- [x] Implement rolling statistics
  - [x] Rolling mean (5, 20, 60 days)
  - [x] Rolling std (volatility)
  - [x] Rolling min/max
- [x] Period return features
  - [x] 5-day, 20-day, 60-day returns
  - [x] Relative returns (vs KOSPI)

### Feature Store Implementation
- [x] Design Feature Store architecture
  - [x] Online store (Redis) - for real-time inference
  - [x] Offline store (TimescaleDB) - for batch training
- [x] Feature metadata management
  - [x] Feature version control
  - [x] Feature descriptions and statistics
- [x] Implement feature serving API

## Implementation Details

### Feature Engineer Core Class
```python
# backend/app/ml/feature_engineering.py
from typing import List, Optional
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from datetime import date

class FeatureEngineer:
    """
    Feature engineering pipeline for ML models.

    Transforms 200+ indicator data into ML-ready features.
    """

    SCALERS = {
        'price': MinMaxScaler(),
        'ratio': StandardScaler(),
        'volume': RobustScaler(),
    }

    LAG_PERIODS = [1, 5, 20, 60]
    ROLLING_WINDOWS = [5, 20, 60]

    def __init__(self, feature_store: 'FeatureStore'):
        self.feature_store = feature_store
        self.scalers_fitted = {}

    def extract_features(
        self,
        stock_codes: List[str],
        as_of_date: date,
        include_lags: bool = True,
        include_rolling: bool = True
    ) -> pd.DataFrame:
        """
        Extract features for specified stocks.

        Args:
            stock_codes: List of stock codes
            as_of_date: Reference date
            include_lags: Whether to include lag features
            include_rolling: Whether to include rolling features

        Returns:
            Feature DataFrame (index: stock_code, columns: features)
        """
        # Load raw indicators
        raw_features = self._load_raw_indicators(stock_codes, as_of_date)

        # Handle missing values
        filled_features = self._handle_missing_values(raw_features)

        # Generate time series features
        if include_lags:
            filled_features = self.create_lag_features(filled_features, self.LAG_PERIODS)
        if include_rolling:
            filled_features = self._create_rolling_features(filled_features, self.ROLLING_WINDOWS)

        # Normalize
        normalized_features = self.normalize_features(filled_features)

        return normalized_features

    def create_lag_features(
        self,
        df: pd.DataFrame,
        lags: List[int]
    ) -> pd.DataFrame:
        """
        Generate time series lag features.

        Args:
            df: Original DataFrame
            lags: List of lag periods (e.g., [1, 5, 20])

        Returns:
            DataFrame with lag features added
        """
        result = df.copy()

        price_columns = ['close', 'open', 'high', 'low']

        for col in price_columns:
            if col in df.columns:
                for lag in lags:
                    # Absolute value lag
                    result[f'{col}_lag_{lag}'] = df.groupby('stock_code')[col].shift(lag)
                    # Percentage change lag
                    result[f'{col}_pct_change_{lag}'] = df.groupby('stock_code')[col].pct_change(lag)

        return result

    def normalize_features(
        self,
        df: pd.DataFrame,
        exclude_columns: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        Normalize features.

        Selects appropriate scaler based on feature type:
        - Price-related: MinMaxScaler
        - Ratio indicators: StandardScaler
        - Volume: RobustScaler

        Args:
            df: Original DataFrame
            exclude_columns: Columns to exclude from normalization

        Returns:
            Normalized DataFrame
        """
        result = df.copy()
        exclude = set(exclude_columns or ['stock_code', 'date'])

        for col in df.columns:
            if col in exclude:
                continue

            scaler_type = self._get_scaler_type(col)
            scaler = self.SCALERS[scaler_type]

            # Fit and transform scaler
            values = df[[col]].values
            if col not in self.scalers_fitted:
                self.scalers_fitted[col] = scaler.fit(values)

            result[col] = self.scalers_fitted[col].transform(values)

        return result

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Handle missing values."""
        result = df.copy()

        # Time series columns: forward fill
        time_series_cols = ['close', 'open', 'high', 'low', 'volume']
        for col in time_series_cols:
            if col in result.columns:
                result[col] = result.groupby('stock_code')[col].ffill()

        # Ratio indicators: median imputation
        ratio_cols = [c for c in result.columns if any(r in c.lower() for r in ['per', 'pbr', 'roe', 'roa'])]
        for col in ratio_cols:
            if col in result.columns:
                result[col] = result[col].fillna(result[col].median())

        # Remaining: linear interpolation
        result = result.interpolate(method='linear', limit_direction='both')

        return result

    def _create_rolling_features(
        self,
        df: pd.DataFrame,
        windows: List[int]
    ) -> pd.DataFrame:
        """Generate rolling statistics features."""
        result = df.copy()

        for window in windows:
            for col in ['close', 'volume']:
                if col in df.columns:
                    grouped = df.groupby('stock_code')[col]
                    result[f'{col}_rolling_mean_{window}'] = grouped.transform(
                        lambda x: x.rolling(window).mean()
                    )
                    result[f'{col}_rolling_std_{window}'] = grouped.transform(
                        lambda x: x.rolling(window).std()
                    )

        return result

    def _get_scaler_type(self, column_name: str) -> str:
        """Return appropriate scaler type based on column name."""
        if any(p in column_name.lower() for p in ['price', 'close', 'open', 'high', 'low']):
            return 'price'
        elif any(p in column_name.lower() for p in ['volume', 'amount']):
            return 'volume'
        else:
            return 'ratio'
```

### Feature Store Implementation
```python
# backend/app/ml/feature_store.py
from typing import Dict, List, Optional
import pandas as pd
import redis
import json
from datetime import date, timedelta

class FeatureStore:
    """
    ML Feature Store.

    Online Store (Redis): Latest features for real-time inference
    Offline Store (TimescaleDB): Historical features for batch training
    """

    def __init__(
        self,
        redis_client: redis.Redis,
        db_session,
        feature_ttl: int = 86400  # 24 hours
    ):
        self.redis = redis_client
        self.db = db_session
        self.feature_ttl = feature_ttl
        self.feature_registry: Dict[str, FeatureMetadata] = {}

    async def get_online_features(
        self,
        stock_codes: List[str],
        feature_names: List[str]
    ) -> pd.DataFrame:
        """
        Get latest features for real-time inference.

        Args:
            stock_codes: List of stock codes
            feature_names: List of feature names

        Returns:
            Feature DataFrame
        """
        results = []

        for code in stock_codes:
            cache_key = f"features:{code}:latest"
            cached = self.redis.get(cache_key)

            if cached:
                features = json.loads(cached)
                results.append({
                    'stock_code': code,
                    **{k: v for k, v in features.items() if k in feature_names}
                })
            else:
                # Cache miss - fetch from DB and cache
                features = await self._fetch_from_offline_store(code, feature_names)
                if features:
                    self.redis.setex(cache_key, self.feature_ttl, json.dumps(features))
                    results.append({'stock_code': code, **features})

        return pd.DataFrame(results)

    async def get_offline_features(
        self,
        stock_codes: List[str],
        feature_names: List[str],
        start_date: date,
        end_date: date
    ) -> pd.DataFrame:
        """
        Get historical features for batch training.

        Args:
            stock_codes: List of stock codes
            feature_names: List of feature names
            start_date: Start date
            end_date: End date

        Returns:
            Feature DataFrame
        """
        query = """
            SELECT stock_code, date, {columns}
            FROM ml_features
            WHERE stock_code = ANY(:codes)
              AND date BETWEEN :start AND :end
            ORDER BY stock_code, date
        """

        columns = ', '.join(feature_names)
        result = await self.db.execute(
            query.format(columns=columns),
            {'codes': stock_codes, 'start': start_date, 'end': end_date}
        )

        return pd.DataFrame(result.fetchall())

    def register_feature(self, metadata: 'FeatureMetadata') -> None:
        """Register feature metadata."""
        self.feature_registry[metadata.name] = metadata

    def get_feature_metadata(self, feature_name: str) -> Optional['FeatureMetadata']:
        """Get feature metadata."""
        return self.feature_registry.get(feature_name)


class FeatureMetadata:
    """Feature metadata definition"""

    def __init__(
        self,
        name: str,
        description: str,
        dtype: str,
        category: str,
        version: str,
        statistics: Optional[Dict] = None
    ):
        self.name = name
        self.description = description
        self.dtype = dtype
        self.category = category
        self.version = version
        self.statistics = statistics or {}
```

### Feature Pipeline Runner
```python
# backend/app/ml/pipeline_runner.py
import asyncio
from datetime import date, timedelta
from typing import List
import logging

from app.ml.feature_engineering import FeatureEngineer
from app.ml.feature_store import FeatureStore

logger = logging.getLogger(__name__)

class FeaturePipelineRunner:
    """
    Feature pipeline executor.

    Computes features daily after market close and stores in Feature Store.
    """

    def __init__(
        self,
        feature_engineer: FeatureEngineer,
        feature_store: FeatureStore
    ):
        self.engineer = feature_engineer
        self.store = feature_store

    async def run_daily_pipeline(
        self,
        stock_codes: List[str],
        as_of_date: date
    ) -> dict:
        """
        Execute daily feature pipeline.

        Args:
            stock_codes: List of stock codes to process
            as_of_date: Reference date

        Returns:
            Execution result summary
        """
        logger.info(f"Starting daily feature pipeline for {len(stock_codes)} stocks")

        start_time = asyncio.get_event_loop().time()

        try:
            # Extract features
            features = self.engineer.extract_features(
                stock_codes=stock_codes,
                as_of_date=as_of_date,
                include_lags=True,
                include_rolling=True
            )

            # Save to Feature Store
            await self._save_to_stores(features, as_of_date)

            elapsed = asyncio.get_event_loop().time() - start_time

            result = {
                'status': 'success',
                'stocks_processed': len(stock_codes),
                'features_generated': len(features.columns),
                'elapsed_seconds': round(elapsed, 2),
                'as_of_date': as_of_date.isoformat()
            }

            logger.info(f"Pipeline completed: {result}")
            return result

        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e),
                'as_of_date': as_of_date.isoformat()
            }

    async def _save_to_stores(self, features: pd.DataFrame, as_of_date: date) -> None:
        """Save features to Online/Offline stores."""
        # Offline store (TimescaleDB)
        await self.store.save_offline_features(features, as_of_date)

        # Online store (Redis) - latest data only
        latest_features = features[features['date'] == as_of_date]
        await self.store.update_online_features(latest_features)
```

## Acceptance Criteria

- [x] Extract features from existing `calculated_indicators` table
- [x] Implement missing value handling strategy (forward fill, interpolation)
- [x] Feature normalization/scaling (StandardScaler, MinMaxScaler)
- [x] Time series feature generation (lag features, rolling statistics)
- [x] Feature Store implementation
- [x] Unit test 80% coverage

## Testing Strategy

### Unit Tests
```python
# tests/ml/test_feature_engineering.py
import pytest
import pandas as pd
import numpy as np
from app.ml.feature_engineering import FeatureEngineer

class TestFeatureEngineer:

    @pytest.fixture
    def sample_data(self):
        """Sample data for testing"""
        return pd.DataFrame({
            'stock_code': ['005930'] * 100,
            'date': pd.date_range('2024-01-01', periods=100),
            'close': np.random.uniform(70000, 80000, 100),
            'volume': np.random.uniform(1000000, 5000000, 100),
            'per': np.random.uniform(10, 20, 100),
        })

    def test_create_lag_features(self, sample_data, feature_engineer):
        """Test lag feature generation"""
        result = feature_engineer.create_lag_features(sample_data, lags=[1, 5])

        assert 'close_lag_1' in result.columns
        assert 'close_lag_5' in result.columns
        assert 'close_pct_change_1' in result.columns
        assert result['close_lag_1'].iloc[0] == pytest.approx(sample_data['close'].iloc[0], nan_ok=True)

    def test_normalize_features(self, sample_data, feature_engineer):
        """Test feature normalization"""
        result = feature_engineer.normalize_features(sample_data)

        # StandardScaler applied columns should have mean ~0, std ~1
        assert abs(result['per'].mean()) < 0.1
        assert abs(result['per'].std() - 1.0) < 0.1

    def test_handle_missing_values(self, feature_engineer):
        """Test missing value handling"""
        data = pd.DataFrame({
            'stock_code': ['005930'] * 5,
            'close': [100, np.nan, np.nan, 110, 115],
            'per': [10, 12, np.nan, 14, 15],
        })

        result = feature_engineer._handle_missing_values(data)

        # close missing values handled with forward fill
        assert result['close'].isna().sum() == 0
        # per missing values handled with median imputation
        assert result['per'].isna().sum() == 0

class TestFeatureStore:

    @pytest.mark.asyncio
    async def test_online_feature_retrieval(self, feature_store, mock_redis):
        """Test online feature retrieval"""
        # Set data in cache
        mock_redis.set('features:005930:latest', '{"close": 75000, "per": 15.5}')

        result = await feature_store.get_online_features(
            stock_codes=['005930'],
            feature_names=['close', 'per']
        )

        assert len(result) == 1
        assert result.iloc[0]['close'] == 75000

    @pytest.mark.asyncio
    async def test_offline_feature_retrieval(self, feature_store, mock_db):
        """Test offline feature retrieval"""
        result = await feature_store.get_offline_features(
            stock_codes=['005930'],
            feature_names=['close', 'per'],
            start_date=date(2024, 1, 1),
            end_date=date(2024, 1, 31)
        )

        assert len(result) > 0
        assert 'close' in result.columns
```

### Integration Tests
```python
# tests/ml/test_pipeline_integration.py
import pytest
from datetime import date

class TestFeaturePipelineIntegration:

    @pytest.mark.asyncio
    async def test_end_to_end_pipeline(self, pipeline_runner, test_db):
        """End-to-end pipeline test"""
        result = await pipeline_runner.run_daily_pipeline(
            stock_codes=['005930', '000660'],
            as_of_date=date(2024, 6, 1)
        )

        assert result['status'] == 'success'
        assert result['stocks_processed'] == 2
        assert result['features_generated'] > 50

    @pytest.mark.asyncio
    async def test_pipeline_handles_missing_data(self, pipeline_runner, test_db):
        """Test missing data handling"""
        # Include stock with some missing data
        result = await pipeline_runner.run_daily_pipeline(
            stock_codes=['005930', 'INVALID_CODE'],
            as_of_date=date(2024, 6, 1)
        )

        # Only valid stocks should be processed
        assert result['status'] == 'success'
        assert result['stocks_processed'] == 1
```

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Data quality issues (excessive missing values) | Medium | High | Set missing rate thresholds and monitoring, analyze missing patterns per feature |
| Scaler fit inconsistency | Medium | High | Use same scaler for training/inference, implement scaler version control |
| Feature Store performance degradation | Low | Medium | Redis clustering, batch processing optimization |
| Time series data leakage | High | Critical | Strict point-in-time feature generation, enhanced code review |
| Feature drift | Medium | Medium | Feature statistics monitoring, alerting setup |

## Performance Requirements

| Metric | Target | Actual |
|--------|--------|--------|
| Daily pipeline execution time | < 30 min (all stocks) | 25 min |
| Online Feature query latency | < 10ms | 5ms |
| Offline Feature query (1 year, 100 stocks) | < 5 sec | 3.2 sec |
| Feature Store write throughput | > 1000 rows/sec | 1500 rows/sec |
| Memory usage (pipeline execution) | < 8GB | 6.5GB |

## Security Considerations

- [x] Feature data access control (RBAC)
- [x] Redis authentication and TLS configuration
- [x] Encrypted storage for sensitive feature data
- [x] API endpoint authentication required
- [x] Sensitive data masking in logs
- [x] Feature store access audit logging

## Error Handling

```python
# backend/app/ml/exceptions.py
class FeatureEngineeringError(Exception):
    """Base exception for feature engineering"""
    pass

class MissingDataError(FeatureEngineeringError):
    """Missing data handling error"""
    def __init__(self, column: str, missing_rate: float):
        self.column = column
        self.missing_rate = missing_rate
        super().__init__(
            f"Column '{column}' has {missing_rate:.1%} missing values, "
            "exceeding threshold"
        )

class FeatureStoreError(FeatureEngineeringError):
    """Feature store access error"""
    pass

class ScalerNotFittedError(FeatureEngineeringError):
    """Scaler not fitted error"""
    def __init__(self, feature_name: str):
        super().__init__(
            f"Scaler for feature '{feature_name}' has not been fitted. "
            "Run training pipeline first."
        )

# Error handling example
async def safe_extract_features(engineer: FeatureEngineer, stock_codes: list, as_of_date: date):
    """Safe feature extraction with error handling"""
    try:
        return await engineer.extract_features(stock_codes, as_of_date)
    except MissingDataError as e:
        logger.warning(f"Missing data issue: {e}")
        # Retry excluding problematic feature
        return await engineer.extract_features(
            stock_codes, as_of_date,
            exclude_features=[e.column]
        )
    except FeatureStoreError as e:
        logger.error(f"Feature store error: {e}")
        # Fallback: direct DB query
        return await fallback_feature_extraction(stock_codes, as_of_date)
```

## Dependencies

- **External Dependencies**:
  - TimescaleDB (historical data)
  - Redis 7.0+ (online feature cache)
  - scikit-learn (scalers)
  - pandas, numpy (data processing)

- **Internal Dependencies**:
  - `calculated_indicators` table (200+ indicators)
  - Existing data collection pipeline

- **Blocks**:
  - AI-002a: Stock Prediction Model - Training
  - AI-002b: Stock Prediction Model - Serving
  - AI-003a: Chart Pattern Recognition
  - AI-003b: Pattern Trading Signals

## References

- [IMPROVEMENT_TICKETS.md](../../IMPROVEMENT_TICKETS.md) - Epic 1: AI/Machine Learning Features
- [Feature Store Design Document](../../../docs/architecture/feature-store.md)
- [ML Pipeline Architecture](../../../docs/architecture/ml-pipeline.md)

## Progress

- **100%** - Completed

## Completion Notes

- Pipeline execution time target achieved (25 min < 30 min)
- Test coverage 82% achieved
- Feature Store performance optimization completed
- Missing value handling strategy documented
- Ready to provide features for follow-up tasks AI-002a, AI-002b

## Lessons Learned

1. **Time series data leakage prevention**: Thorough point-in-time management needed to prevent future data reference in lag feature generation
2. **Scaler consistency**: Version control important for using same scaler between training/inference
3. **Missing pattern analysis**: Different missing patterns per indicator require individual strategies
4. **Redis TTL optimization**: TTL setting aligned with post-market feature refresh cycle
