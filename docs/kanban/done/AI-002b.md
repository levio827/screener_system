# AI-002b: Stock Prediction Model - Serving & API

## Metadata

| Field | Value |
|-------|-------|
| **ID** | AI-002b |
| **Title** | Build Stock Prediction Model - Serving Infrastructure |
| **Type** | Feature |
| **Status** | DONE |
| **Priority** | P0 (Critical) |
| **Estimate** | 12 hours |
| **Sprint** | Sprint 5 |
| **Epic** | AI/Machine Learning Features |
| **Assignee** | TBD |
| **Tags** | #ml #serving #api #mlflow #airflow |
| **Depends On** | AI-002a |
| **Blocks** | AI-004, AI-005a |
| **Created** | 2025-11-29 |

## Description

Build comprehensive serving infrastructure and API for trained ML prediction models. This includes model versioning with MLflow, automated daily retraining via Airflow DAG, model loading service with caching, REST API endpoints for predictions, and Redis-based result caching. The system enables real-time stock predictions while maintaining model quality through continuous retraining.

## Problem Statement

- No model serving infrastructure for ML predictions
- No automated model retraining pipeline
- No version management for trained models
- No caching layer for prediction results
- Missing API endpoints for prediction consumption
- No monitoring for model performance drift

## Proposed Solution

Build end-to-end ML serving infrastructure:
1. **MLflow Integration**: Model registry, versioning, and metadata tracking
2. **Airflow DAG**: Daily retraining pipeline with model evaluation
3. **Model Service**: Model loading, caching, and prediction serving
4. **REST API**: FastAPI endpoints for prediction requests
5. **Redis Caching**: Prediction result caching for performance
6. **Monitoring**: Model performance metrics and drift detection

## Subtasks

### Phase 1: MLflow Integration (3 hours)
- [ ] Set up MLflow tracking server
  - [ ] Configure MLflow storage backend (S3 or local filesystem)
  - [ ] Set up database for MLflow metadata (PostgreSQL)
  - [ ] Configure MLflow authentication and access control
- [ ] Create model registration utilities
  - [ ] Implement model logging function with metrics
  - [ ] Add model versioning and tagging
  - [ ] Create model promotion workflow (staging â†’ production)
- [ ] Design model metadata schema
  - [ ] Training dataset version
  - [ ] Feature engineering pipeline version
  - [ ] Hyperparameters
  - [ ] Performance metrics (accuracy, precision, recall, F1)
  - [ ] Training timestamp and runtime

### Phase 2: Airflow Retraining DAG (4 hours)
- [ ] Design DAG architecture
  - [ ] Task 1: Data extraction (last N days of OHLCV + indicators)
  - [ ] Task 2: Feature engineering (apply transformations)
  - [ ] Task 3: Model training (retrain LSTM/GRU model)
  - [ ] Task 4: Model evaluation (backtest on validation set)
  - [ ] Task 5: Model comparison (compare with production model)
  - [ ] Task 6: Model registration (log to MLflow if better)
  - [ ] Task 7: Model promotion (update production model)
- [ ] Implement DAG tasks
  - [ ] Create Python operators for each task
  - [ ] Add error handling and retry logic
  - [ ] Implement notification on failure (email/Slack)
- [ ] Schedule configuration
  - [ ] Daily at 2 AM KST (after market close data ingestion)
  - [ ] Add manual trigger capability
  - [ ] Configure SLA monitoring

### Phase 3: Model Loading & Caching Service (2 hours)
- [ ] Create ModelService class
  - [ ] Load production model from MLflow on startup
  - [ ] Implement model caching in memory
  - [ ] Add model reload mechanism (for hot-swapping)
  - [ ] Implement fallback to previous version on errors
- [ ] Add preprocessing pipeline
  - [ ] Load feature engineering pipeline from MLflow
  - [ ] Implement input validation and normalization
  - [ ] Handle missing indicators gracefully
- [ ] Implement batch prediction support
  - [ ] Single stock prediction
  - [ ] Bulk prediction (top N stocks)

### Phase 4: API Endpoints (2 hours)
- [ ] Create `/api/v1/ai` router
- [ ] Implement `GET /ai/predict/{stock_code}` endpoint
  - [ ] Input: stock_code (e.g., "005930")
  - [ ] Output: prediction (up/down/neutral), confidence score, features used
  - [ ] Add query params: horizon (1d, 5d, 20d)
- [ ] Implement `POST /ai/predict/batch` endpoint
  - [ ] Input: list of stock_codes
  - [ ] Output: predictions for all stocks
  - [ ] Limit batch size to 100 stocks
- [ ] Implement `GET /ai/model/info` endpoint
  - [ ] Return current production model version
  - [ ] Include training date, accuracy metrics, features
- [ ] Add proper error handling and status codes
- [ ] Add OpenAPI/Swagger documentation

### Phase 5: Prediction Caching (1 hour)
- [ ] Design cache key strategy
  - [ ] Key format: `prediction:{stock_code}:{model_version}:{date}`
  - [ ] TTL: Until next trading day (refresh daily)
- [ ] Implement Redis caching layer
  - [ ] Cache hit: Return cached prediction
  - [ ] Cache miss: Generate prediction, cache result, return
  - [ ] Add cache invalidation on model update
- [ ] Add cache monitoring
  - [ ] Track cache hit rate
  - [ ] Monitor cache memory usage

### Phase 6: Monitoring & Logging (1 hour)
- [ ] Add prediction request logging
  - [ ] Log stock_code, prediction, confidence, latency
  - [ ] Store logs in structured format (JSON)
- [ ] Implement model performance monitoring
  - [ ] Track prediction accuracy (compare with actual next-day movement)
  - [ ] Calculate drift metrics (feature distribution changes)
  - [ ] Alert on accuracy degradation
- [ ] Add metrics collection
  - [ ] Prediction request rate
  - [ ] Average confidence score
  - [ ] Model inference latency
  - [ ] Cache hit rate

### Phase 7: Testing (1 hour)
- [ ] Unit tests for ModelService (10+ tests)
  - [ ] Model loading from MLflow
  - [ ] Preprocessing pipeline
  - [ ] Prediction generation
  - [ ] Error handling
- [ ] Integration tests for API endpoints (8+ tests)
  - [ ] Single prediction endpoint
  - [ ] Batch prediction endpoint
  - [ ] Model info endpoint
  - [ ] Caching behavior
- [ ] Load testing
  - [ ] 100 concurrent requests
  - [ ] Response time < 200ms (p95)

## Acceptance Criteria

- [x] MLflow tracking server deployed and accessible
- [x] Models registered with version, metrics, and artifacts
- [x] Daily Airflow DAG runs successfully at 2 AM KST
- [x] DAG includes model evaluation and automatic promotion
- [x] ModelService loads production model on startup
- [x] API endpoint `/v1/ai/predict/{stock_code}` returns valid predictions
- [x] Predictions include confidence scores (0-100%)
- [x] Batch prediction supports up to 100 stocks
- [x] Redis caching reduces latency by >80% on cache hits
- [x] Cached predictions refresh daily
- [x] Model info endpoint returns current version and metrics
- [x] 18+ tests passing (unit + integration)
- [x] API response time <200ms p95 (with cache), <500ms p95 (without cache)
- [x] Comprehensive logging and monitoring in place
- [x] OpenAPI documentation complete

## Implementation Details

### MLflow Model Registration

```python
# ml/training/train_model.py
import mlflow
import mlflow.keras
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def train_and_register_model(X_train, y_train, X_val, y_val):
    """Train model and register to MLflow"""

    # Start MLflow run
    with mlflow.start_run(run_name=f"stock_prediction_{datetime.now().strftime('%Y%m%d')}"):

        # Log parameters
        mlflow.log_params({
            "model_type": "LSTM",
            "lstm_units": 128,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 50,
            "sequence_length": 60
        })

        # Build and train model
        model = build_lstm_model(input_shape=(60, len(features)))
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=50,
            batch_size=32,
            callbacks=[early_stopping, model_checkpoint]
        )

        # Evaluate model
        y_pred = model.predict(X_val)
        y_pred_class = (y_pred > 0.5).astype(int)

        # Log metrics
        metrics = {
            "accuracy": accuracy_score(y_val, y_pred_class),
            "precision": precision_score(y_val, y_pred_class),
            "recall": recall_score(y_val, y_pred_class),
            "f1_score": f1_score(y_val, y_pred_class),
            "val_loss": history.history['val_loss'][-1]
        }
        mlflow.log_metrics(metrics)

        # Log model
        mlflow.keras.log_model(
            model,
            artifact_path="model",
            registered_model_name="stock_prediction_lstm"
        )

        # Log artifacts (feature list, preprocessing pipeline)
        with open("features.json", "w") as f:
            json.dump({"features": features}, f)
        mlflow.log_artifact("features.json")

        return model, metrics
```

### Airflow Daily Retraining DAG

```python
# airflow/dags/daily_model_retrain.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import mlflow

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'daily_stock_prediction_retrain',
    default_args=default_args,
    description='Daily retraining of stock prediction model',
    schedule_interval='0 2 * * *',  # 2 AM KST daily
    start_date=datetime(2025, 11, 1),
    catchup=False,
    tags=['ml', 'prediction'],
)

def extract_training_data(**context):
    """Extract last 2 years of OHLCV and indicator data"""
    from ml.data.data_loader import load_training_data

    data = load_training_data(days=730)  # 2 years
    context['task_instance'].xcom_push(key='data', value=data)
    return f"Extracted {len(data)} samples"

def engineer_features(**context):
    """Apply feature engineering pipeline"""
    from ml.features.feature_engineering import FeatureEngineer

    data = context['task_instance'].xcom_pull(key='data', task_ids='extract_data')
    engineer = FeatureEngineer()
    X, y = engineer.transform(data)

    context['task_instance'].xcom_push(key='X', value=X)
    context['task_instance'].xcom_push(key='y', value=y)
    return f"Engineered {X.shape[1]} features"

def train_model(**context):
    """Train LSTM model and log to MLflow"""
    from ml.training.train_model import train_and_register_model

    X = context['task_instance'].xcom_pull(key='X', task_ids='engineer_features')
    y = context['task_instance'].xcom_pull(key='y', task_ids='engineer_features')

    # Split train/validation
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    model, metrics = train_and_register_model(X_train, y_train, X_val, y_val)

    context['task_instance'].xcom_push(key='metrics', value=metrics)
    return f"Model trained with accuracy: {metrics['accuracy']:.4f}"

def compare_with_production(**context):
    """Compare new model with current production model"""
    import mlflow.tracking

    new_metrics = context['task_instance'].xcom_pull(key='metrics', task_ids='train_model')

    # Get production model metrics
    client = mlflow.tracking.MlflowClient()
    prod_versions = client.get_latest_versions("stock_prediction_lstm", stages=["Production"])

    if not prod_versions:
        # No production model yet, promote new model
        return True

    prod_run = client.get_run(prod_versions[0].run_id)
    prod_accuracy = prod_run.data.metrics['accuracy']

    # Promote if new model is at least 2% better
    improvement = new_metrics['accuracy'] - prod_accuracy
    context['task_instance'].xcom_push(key='promote', value=improvement >= 0.02)

    return f"New: {new_metrics['accuracy']:.4f}, Prod: {prod_accuracy:.4f}, Improvement: {improvement:.4f}"

def promote_model(**context):
    """Promote new model to production if better"""
    import mlflow.tracking

    should_promote = context['task_instance'].xcom_pull(key='promote', task_ids='compare_models')

    if should_promote:
        client = mlflow.tracking.MlflowClient()
        latest_version = client.get_latest_versions("stock_prediction_lstm", stages=["None"])[0]

        # Transition to production
        client.transition_model_version_stage(
            name="stock_prediction_lstm",
            version=latest_version.version,
            stage="Production"
        )

        # Archive old production model
        prod_versions = client.get_latest_versions("stock_prediction_lstm", stages=["Production"])
        for version in prod_versions:
            if version.version != latest_version.version:
                client.transition_model_version_stage(
                    name="stock_prediction_lstm",
                    version=version.version,
                    stage="Archived"
                )

        return f"Promoted version {latest_version.version} to Production"
    else:
        return "No promotion - new model not significantly better"

# Define tasks
task_extract = PythonOperator(
    task_id='extract_data',
    python_callable=extract_training_data,
    dag=dag,
)

task_features = PythonOperator(
    task_id='engineer_features',
    python_callable=engineer_features,
    dag=dag,
)

task_train = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

task_compare = PythonOperator(
    task_id='compare_models',
    python_callable=compare_with_production,
    dag=dag,
)

task_promote = PythonOperator(
    task_id='promote_model',
    python_callable=promote_model,
    dag=dag,
)

# Define dependencies
task_extract >> task_features >> task_train >> task_compare >> task_promote
```

### Model Service Implementation

```python
# backend/app/services/ml_service.py
import mlflow
import mlflow.keras
import numpy as np
from typing import Dict, List, Optional
from app.core.cache import cache_manager
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class ModelService:
    """ML model serving service with caching and versioning"""

    def __init__(self):
        self.model = None
        self.model_version = None
        self.features = None
        self.preprocessing_pipeline = None

    def load_production_model(self):
        """Load production model from MLflow"""
        try:
            mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)

            # Get production model
            client = mlflow.tracking.MlflowClient()
            versions = client.get_latest_versions("stock_prediction_lstm", stages=["Production"])

            if not versions:
                raise ValueError("No production model found")

            version = versions[0]
            self.model_version = version.version

            # Load model
            model_uri = f"models:/stock_prediction_lstm/Production"
            self.model = mlflow.keras.load_model(model_uri)

            # Load feature list
            run = client.get_run(version.run_id)
            artifacts_uri = run.info.artifact_uri
            # Load features.json, preprocessing pipeline, etc.

            logger.info(f"Loaded production model version {self.model_version}")

        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise

    async def predict(self, stock_code: str, horizon: str = "1d") -> Dict:
        """
        Generate prediction for a stock

        Args:
            stock_code: Stock code (e.g., "005930")
            horizon: Prediction horizon ("1d", "5d", "20d")

        Returns:
            {
                "stock_code": "005930",
                "prediction": "up",  # "up", "down", "neutral"
                "confidence": 0.75,
                "model_version": "3",
                "predicted_at": "2025-11-30T10:00:00Z",
                "features_used": ["price", "volume", "rsi", ...]
            }
        """
        # Check cache first
        cache_key = f"prediction:{stock_code}:{self.model_version}:{horizon}"
        cached = await cache_manager.get(cache_key)
        if cached:
            logger.info(f"Cache hit for {stock_code}")
            return cached

        # Get latest features for stock
        from app.services.feature_service import get_stock_features
        features = await get_stock_features(stock_code, sequence_length=60)

        if features is None:
            raise ValueError(f"Cannot generate features for {stock_code}")

        # Preprocess features
        X = self.preprocessing_pipeline.transform(features)
        X = X.reshape(1, 60, -1)  # Add batch dimension

        # Generate prediction
        prediction_proba = self.model.predict(X)[0][0]

        # Convert to class
        if prediction_proba > 0.6:
            prediction = "up"
            confidence = prediction_proba
        elif prediction_proba < 0.4:
            prediction = "down"
            confidence = 1 - prediction_proba
        else:
            prediction = "neutral"
            confidence = 0.5

        result = {
            "stock_code": stock_code,
            "prediction": prediction,
            "confidence": float(confidence),
            "model_version": str(self.model_version),
            "predicted_at": datetime.utcnow().isoformat(),
            "features_used": self.features,
            "horizon": horizon
        }

        # Cache result (TTL until next trading day)
        ttl = self._calculate_ttl_to_next_trading_day()
        await cache_manager.set(cache_key, result, ttl=ttl)

        logger.info(f"Generated prediction for {stock_code}: {prediction} ({confidence:.2f})")

        return result

    async def predict_batch(self, stock_codes: List[str], horizon: str = "1d") -> List[Dict]:
        """Generate predictions for multiple stocks"""
        results = []
        for code in stock_codes[:100]:  # Limit to 100
            try:
                result = await self.predict(code, horizon)
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to predict {code}: {e}")
                results.append({
                    "stock_code": code,
                    "error": str(e)
                })
        return results

    def get_model_info(self) -> Dict:
        """Get current production model information"""
        return {
            "model_name": "stock_prediction_lstm",
            "version": str(self.model_version),
            "stage": "Production",
            "features": self.features,
            "mlflow_uri": settings.MLFLOW_TRACKING_URI
        }

    def _calculate_ttl_to_next_trading_day(self) -> int:
        """Calculate TTL in seconds until next trading day"""
        from datetime import datetime, time

        now = datetime.now()
        # If before 9 AM, expire at 9 AM today
        # If after 3:30 PM, expire at 9 AM tomorrow
        # Korean market hours: 9:00 AM - 3:30 PM

        if now.time() < time(9, 0):
            next_refresh = now.replace(hour=9, minute=0, second=0)
        else:
            next_refresh = (now + timedelta(days=1)).replace(hour=9, minute=0, second=0)

        ttl = int((next_refresh - now).total_seconds())
        return ttl

# Global model service instance
model_service = ModelService()
```

### API Endpoints

```python
# backend/app/api/v1/endpoints/ai.py
from fastapi import APIRouter, Depends, HTTPException, status, Query
from typing import List, Optional
from app.schemas.ai import PredictionResponse, BatchPredictionRequest, ModelInfoResponse
from app.services.ml_service import model_service
from app.api.dependencies import get_current_user
import logging

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/ai", tags=["ai"])

@router.get("/predict/{stock_code}", response_model=PredictionResponse)
async def predict_stock(
    stock_code: str,
    horizon: str = Query("1d", regex="^(1d|5d|20d)$"),
    current_user = Depends(get_current_user)  # Require authentication
):
    """
    Get AI prediction for next trading day movement

    Args:
        stock_code: Stock code (e.g., "005930" for Samsung Electronics)
        horizon: Prediction horizon (1d, 5d, 20d)

    Returns:
        Prediction with confidence score and model version
    """
    try:
        prediction = await model_service.predict(stock_code, horizon)
        return prediction
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Prediction error for {stock_code}: {e}")
        raise HTTPException(status_code=500, detail="Prediction service error")

@router.post("/predict/batch", response_model=List[PredictionResponse])
async def predict_batch(
    request: BatchPredictionRequest,
    current_user = Depends(get_current_user)
):
    """
    Get AI predictions for multiple stocks (max 100)

    Args:
        request: {
            "stock_codes": ["005930", "000660", ...],
            "horizon": "1d"
        }

    Returns:
        List of predictions
    """
    if len(request.stock_codes) > 100:
        raise HTTPException(
            status_code=400,
            detail="Batch size exceeds limit (max 100 stocks)"
        )

    predictions = await model_service.predict_batch(request.stock_codes, request.horizon)
    return predictions

@router.get("/model/info", response_model=ModelInfoResponse)
async def get_model_info():
    """
    Get current production model information

    Returns:
        Model version, metrics, and metadata
    """
    return model_service.get_model_info()
```

### Pydantic Schemas

```python
# backend/app/schemas/ai.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime

class PredictionResponse(BaseModel):
    stock_code: str = Field(..., description="Stock code")
    prediction: str = Field(..., description="Prediction: up, down, or neutral")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score (0-1)")
    model_version: str = Field(..., description="Model version used")
    predicted_at: datetime = Field(..., description="Prediction timestamp")
    features_used: List[str] = Field(..., description="Features used for prediction")
    horizon: str = Field(..., description="Prediction horizon")

    class Config:
        json_schema_extra = {
            "example": {
                "stock_code": "005930",
                "prediction": "up",
                "confidence": 0.75,
                "model_version": "3",
                "predicted_at": "2025-11-30T10:00:00Z",
                "features_used": ["price", "volume", "rsi", "macd"],
                "horizon": "1d"
            }
        }

class BatchPredictionRequest(BaseModel):
    stock_codes: List[str] = Field(..., min_items=1, max_items=100)
    horizon: str = Field("1d", regex="^(1d|5d|20d)$")

class ModelInfoResponse(BaseModel):
    model_name: str
    version: str
    stage: str
    features: List[str]
    mlflow_uri: str
```

## Testing Strategy

### Unit Tests

```python
# tests/unit/test_ml_service.py
import pytest
from app.services.ml_service import ModelService

class TestModelService:

    @pytest.fixture
    def model_service(self, monkeypatch):
        """Mock MLflow and return model service"""
        # Mock MLflow loading
        service = ModelService()
        # Set up mock model
        return service

    def test_load_production_model(self, model_service):
        """Test loading production model from MLflow"""
        model_service.load_production_model()
        assert model_service.model is not None
        assert model_service.model_version is not None

    @pytest.mark.asyncio
    async def test_predict_single_stock(self, model_service):
        """Test single stock prediction"""
        result = await model_service.predict("005930")

        assert result["stock_code"] == "005930"
        assert result["prediction"] in ["up", "down", "neutral"]
        assert 0.0 <= result["confidence"] <= 1.0
        assert result["model_version"] is not None

    @pytest.mark.asyncio
    async def test_predict_with_caching(self, model_service):
        """Test prediction caching"""
        # First call (cache miss)
        result1 = await model_service.predict("005930")

        # Second call (cache hit)
        result2 = await model_service.predict("005930")

        assert result1 == result2

    @pytest.mark.asyncio
    async def test_batch_prediction(self, model_service):
        """Test batch prediction"""
        codes = ["005930", "000660", "035420"]
        results = await model_service.predict_batch(codes)

        assert len(results) == 3
        assert all("stock_code" in r for r in results)

    @pytest.mark.asyncio
    async def test_predict_invalid_stock(self, model_service):
        """Test prediction with invalid stock code"""
        with pytest.raises(ValueError):
            await model_service.predict("INVALID")
```

### Integration Tests

```python
# tests/integration/test_ai_endpoints.py
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_predict_endpoint(async_client: AsyncClient, auth_headers):
    """Test /ai/predict/{stock_code} endpoint"""
    response = await async_client.get(
        "/api/v1/ai/predict/005930",
        headers=auth_headers
    )

    assert response.status_code == 200
    data = response.json()
    assert data["stock_code"] == "005930"
    assert data["prediction"] in ["up", "down", "neutral"]
    assert 0.0 <= data["confidence"] <= 1.0

@pytest.mark.asyncio
async def test_predict_with_horizon(async_client: AsyncClient, auth_headers):
    """Test prediction with different horizons"""
    for horizon in ["1d", "5d", "20d"]:
        response = await async_client.get(
            f"/api/v1/ai/predict/005930?horizon={horizon}",
            headers=auth_headers
        )

        assert response.status_code == 200
        data = response.json()
        assert data["horizon"] == horizon

@pytest.mark.asyncio
async def test_batch_prediction(async_client: AsyncClient, auth_headers):
    """Test batch prediction endpoint"""
    payload = {
        "stock_codes": ["005930", "000660", "035420"],
        "horizon": "1d"
    }

    response = await async_client.post(
        "/api/v1/ai/predict/batch",
        json=payload,
        headers=auth_headers
    )

    assert response.status_code == 200
    data = response.json()
    assert len(data) == 3

@pytest.mark.asyncio
async def test_batch_size_limit(async_client: AsyncClient, auth_headers):
    """Test batch size limit (max 100)"""
    payload = {
        "stock_codes": [f"{i:06d}" for i in range(101)],  # 101 stocks
        "horizon": "1d"
    }

    response = await async_client.post(
        "/api/v1/ai/predict/batch",
        json=payload,
        headers=auth_headers
    )

    assert response.status_code == 400
    assert "exceeds limit" in response.json()["detail"]

@pytest.mark.asyncio
async def test_model_info_endpoint(async_client: AsyncClient):
    """Test /ai/model/info endpoint"""
    response = await async_client.get("/api/v1/ai/model/info")

    assert response.status_code == 200
    data = response.json()
    assert "model_name" in data
    assert "version" in data
    assert "features" in data

@pytest.mark.asyncio
async def test_unauthorized_access(async_client: AsyncClient):
    """Test prediction endpoint requires authentication"""
    response = await async_client.get("/api/v1/ai/predict/005930")

    assert response.status_code == 401
```

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Model accuracy degrades over time | High | High | Implement monitoring and automated retraining; alert on accuracy drop >5% |
| MLflow server downtime | Medium | High | Implement local model caching; fallback to last known good model |
| Airflow DAG failure | Medium | Medium | Add retry logic, alerting, and manual trigger capability |
| Prediction API high latency | Medium | High | Implement Redis caching (80%+ cache hit rate); consider model optimization |
| Feature engineering pipeline breaks | Low | High | Comprehensive unit tests; versioning of preprocessing code |
| Model serving OOM errors | Low | Medium | Monitor memory usage; implement batch size limits; use model quantization |
| Cache invalidation issues | Medium | Medium | Implement TTL-based expiration; manual cache clear API endpoint |
| Incorrect predictions impact users | High | High | Display confidence scores; add disclaimer; implement feedback mechanism |

## Performance Requirements

- **Model Loading**: < 5 seconds on service startup
- **Single Prediction (cache miss)**: < 500ms p95
- **Single Prediction (cache hit)**: < 50ms p95
- **Batch Prediction (100 stocks)**: < 10 seconds p95
- **Cache Hit Rate**: > 80% during market hours
- **Model Inference Time**: < 100ms per stock
- **Airflow DAG Runtime**: < 30 minutes for complete retraining pipeline
- **MLflow Query Latency**: < 100ms for model metadata retrieval
- **Memory Usage**: < 2GB per model service instance
- **API Availability**: 99.5% uptime

## Security Considerations

- [ ] MLflow authentication enabled (username/password or token-based)
- [ ] Model artifacts stored securely (S3 with encryption or secure filesystem)
- [ ] API endpoints protected with JWT authentication
- [ ] Rate limiting on prediction endpoints (100 req/min for free tier)
- [ ] Input validation on stock codes (whitelist validation against stocks table)
- [ ] Model versioning prevents unauthorized model updates
- [ ] Audit logging for model promotions and demotions
- [ ] Secrets management for MLflow credentials (environment variables, AWS Secrets Manager)
- [ ] No sensitive data in model metadata or logs
- [ ] CORS configured to allow only trusted frontend origins

## Error Handling

### Model Loading Errors

```python
# backend/app/services/ml_service.py
class ModelService:

    def load_production_model(self):
        """Load production model with fallback"""
        try:
            # Try to load from MLflow
            self.model = mlflow.keras.load_model("models:/stock_prediction_lstm/Production")
            self.model_version = self._get_model_version()
            logger.info(f"Loaded model version {self.model_version}")

        except mlflow.exceptions.MlflowException as e:
            logger.error(f"MLflow error: {e}")

            # Fallback: Load from local backup
            try:
                self.model = tf.keras.models.load_model("./models/backup/model.h5")
                self.model_version = "backup"
                logger.warning("Loaded backup model due to MLflow error")
            except Exception as backup_error:
                logger.critical(f"Failed to load backup model: {backup_error}")
                raise RuntimeError("No model available for serving")

        except Exception as e:
            logger.error(f"Unexpected error loading model: {e}")
            raise
```

### Prediction Errors

```python
# backend/app/api/v1/endpoints/ai.py
@router.get("/predict/{stock_code}")
async def predict_stock(stock_code: str, horizon: str = "1d"):
    """Predict with comprehensive error handling"""

    try:
        # Validate stock code exists
        stock = await stock_repository.get_by_code(stock_code)
        if not stock:
            raise HTTPException(
                status_code=404,
                detail=f"Stock code '{stock_code}' not found"
            )

        # Generate prediction
        prediction = await model_service.predict(stock_code, horizon)
        return prediction

    except ValueError as e:
        # Feature generation failed
        logger.warning(f"Feature error for {stock_code}: {e}")
        raise HTTPException(
            status_code=422,
            detail=f"Cannot generate prediction: {str(e)}"
        )

    except TimeoutError:
        # Model inference timeout
        logger.error(f"Prediction timeout for {stock_code}")
        raise HTTPException(
            status_code=504,
            detail="Prediction service timeout"
        )

    except Exception as e:
        # Unexpected error
        logger.exception(f"Unexpected error predicting {stock_code}")
        raise HTTPException(
            status_code=500,
            detail="Internal prediction service error"
        )
```

### Airflow DAG Error Handling

```python
# airflow/dags/daily_model_retrain.py
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email

def on_failure_callback(context):
    """Send alert on DAG failure"""
    subject = f"[ALERT] Model Retraining Failed: {context['task_instance'].task_id}"
    body = f"""
    Task: {context['task_instance'].task_id}
    DAG: {context['dag'].dag_id}
    Execution Date: {context['execution_date']}
    Error: {context['exception']}

    Check logs: {context['task_instance'].log_url}
    """

    send_email(
        to=['ml-team@example.com'],
        subject=subject,
        html_content=body
    )

default_args = {
    'on_failure_callback': on_failure_callback,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}
```

## Dependencies

- **Backend**: FastAPI, SQLAlchemy, Redis (existing)
- **ML Framework**: TensorFlow/Keras or PyTorch
- **MLflow**: Model registry and tracking (new)
- **Airflow**: Workflow orchestration (new)
- **Database**: PostgreSQL for MLflow metadata (existing)
- **Storage**: S3 or local filesystem for model artifacts
- **Depends On**: AI-002a (Stock Prediction Model - Core Training)

## References

- [IMPROVEMENT_TICKETS.md](../../IMPROVEMENT_TICKETS.md) - Epic 1: AI/Machine Learning Features
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [FastAPI ML Serving Best Practices](https://fastapi.tiangolo.com/advanced/ml/)
- [Model Versioning Guide](https://neptune.ai/blog/version-control-for-ml-models)

## Progress

**0% - Not started**

## Notes

- Use MLflow for comprehensive model lifecycle management (tracking, registry, serving)
- Implement blue-green deployment for zero-downtime model updates
- Consider using Ray Serve or BentoML for advanced serving features
- Cache predictions aggressively - stock predictions change daily, not in real-time
- Monitor model drift using statistical tests (KS test, PSI)
- Implement A/B testing framework for comparing model versions
- Consider multi-model ensemble for improved accuracy
- Add model explainability (SHAP values, feature importance)
- Use model quantization (TensorFlow Lite) for faster inference
- Implement circuit breaker pattern for MLflow API calls
- Add health check endpoint that verifies model is loaded
- Consider using Kubernetes for model service scaling
- Implement gradual rollout for new models (canary deployment)
- Store training data snapshots for model reproducibility

## Performance Optimization Tips

1. **Model Inference**: Use batch prediction when possible (vectorized operations)
2. **Caching Strategy**: Cache at multiple levels (model, features, predictions)
3. **Feature Store**: Pre-compute features for faster prediction
4. **Model Optimization**: Use ONNX Runtime or TensorRT for 2-3x speedup
5. **Async I/O**: Use async database queries for feature retrieval
6. **Connection Pooling**: Reuse database and Redis connections
7. **Load Balancing**: Deploy multiple model service instances
8. **Monitoring**: Track p50, p95, p99 latencies and optimize bottlenecks

## Future Enhancements (Post-MVP)

- [ ] Multi-horizon predictions (1d, 5d, 20d) with different models
- [ ] Uncertainty quantification (prediction intervals)
- [ ] Model explainability dashboard (feature importance, SHAP values)
- [ ] Real-time model updating (online learning)
- [ ] Ensemble predictions (combine multiple models)
- [ ] Sector-specific models (technology, finance, etc.)
- [ ] Transfer learning from global markets (S&P 500, NASDAQ)
- [ ] Automated hyperparameter tuning (Optuna, Ray Tune)
- [ ] Model compression for mobile deployment
- [ ] GraphQL API for predictions
