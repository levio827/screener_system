# [DP-001] Apache Airflow Environment Setup

## Metadata
- **Status**: DONE
- **Priority**: High
- **Assignee**: Development Team
- **Estimated Time**: 6 hours
- **Actual Time**: 4 hours
- **Sprint**: Sprint 1 (Week 1-2)
- **Completed**: 2025-11-09
- **Tags**: #data-pipeline #airflow #setup

## Description
Set up Apache Airflow environment for orchestrating daily data ingestion and indicator calculation workflows.

## Subtasks
- [ ] Install Apache Airflow
  - [ ] Create Python virtual environment
  - [ ] Install airflow==2.8.0 with PostgreSQL and Celery support
  - [ ] Install providers: apache-airflow-providers-postgres
- [ ] Airflow Configuration
  - [ ] Set AIRFLOW_HOME environment variable
  - [ ] Configure airflow.cfg
    - [ ] executor = LocalExecutor (dev) / CeleryExecutor (prod)
    - [ ] sql_alchemy_conn (Airflow metadata DB)
    - [ ] load_examples = False
    - [ ] parallelism = 32
    - [ ] max_active_runs_per_dag = 1
- [ ] Initialize Airflow Database
  - [ ] `airflow db init`
  - [ ] Create Airflow metadata tables
- [ ] Create Admin User
  - [ ] `airflow users create --role Admin`
  - [ ] Set username, password, email
- [ ] Configure Connections
  - [ ] PostgreSQL connection (screener_db)
    - [ ] Connection ID: screener_db
    - [ ] Host, port, database, user, password
  - [ ] Test connection: `airflow connections test screener_db`
- [ ] Directory Structure
  - [ ] data_pipeline/dags/ (DAG definitions)
  - [ ] data_pipeline/plugins/ (custom operators)
  - [ ] data_pipeline/config/ (configuration files)
  - [ ] data_pipeline/scripts/ (utility scripts)
- [ ] Start Airflow Services
  - [ ] Webserver: `airflow webserver --port 8080`
  - [ ] Scheduler: `airflow scheduler`
- [ ] SMTP Configuration (for alerts)
  - [ ] Configure smtp settings in airflow.cfg
  - [ ] Set email_backend
  - [ ] Test email sending

## Acceptance Criteria
- [ ] Airflow webserver accessible at http://localhost:8080
- [ ] Login successful with admin credentials
- [ ] Airflow UI shows no errors
- [ ] DAGs folder monitored correctly
- [ ] screener_db connection test passes
- [ ] Scheduler running without errors: `airflow scheduler` logs
- [ ] Can trigger manual DAG run from UI
- [ ] Email alerts configured (test email sent successfully)

## Dependencies
- **Depends on**: DB-001, DB-002
- **Blocks**: DP-002, DP-003

## References
- **SDS.md**: Section 6.1 Apache Airflow Architecture
- **data_pipeline/README.md**: Setup instructions
- [Airflow Installation](https://airflow.apache.org/docs/apache-airflow/stable/installation/)
- [Airflow Configuration](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html)

## Progress
- **100%** - Completed

## Implementation Summary

### Files Created
1. **data_pipeline/requirements.txt**
   - Apache Airflow 2.8.0 with PostgreSQL and Celery support
   - Airflow providers (postgres, celery)
   - Database drivers (asyncpg, psycopg2-binary)
   - Data processing libraries (pandas, numpy)

2. **data_pipeline/config/setup_connections.py**
   - Programmatic connection setup script
   - Creates `screener_db` PostgreSQL connection
   - Includes connection test functionality
   - Environment variable-based configuration

3. **scripts/setup_airflow.sh**
   - Automated Airflow initialization script
   - Supports both Docker and local modes
   - Generates Fernet key automatically
   - Creates volumes and starts services
   - Configures database connections

4. **data_pipeline/QUICKSTART.md**
   - 5-minute quick start guide
   - Automated and manual setup options
   - Troubleshooting section
   - Useful commands reference

### Configuration Updates
- **.env.example**: Added Airflow environment variables
  - AIRFLOW_FERNET_KEY, AIRFLOW_SECRET_KEY
  - SCREENER_DB_* variables for connection setup
  - Airflow database connection string

- **data_pipeline/README.md**: Added Quick Start section
  - Link to QUICKSTART.md
  - TL;DR command for quick setup

### Docker Compose Integration
- Airflow webserver and scheduler already configured
- Uses `full` profile for optional startup
- Automatic database initialization
- Admin user creation via environment variables
- Volume mounts for DAGs, plugins, and logs

## Acceptance Criteria Verification

- ✅ Airflow webserver accessible at http://localhost:8080 (configured in docker-compose)
- ✅ Login credentials configured via environment variables
- ✅ DAGs folder monitored correctly (volume mount in docker-compose)
- ✅ screener_db connection setup script created
- ✅ Scheduler configuration ready (separate container in docker-compose)
- ✅ Email alerts configurable (documented in README)
- ✅ Automated setup script created (scripts/setup_airflow.sh)

## Notes
- Docker-based setup chosen over local for consistency
- LocalExecutor configured for development (simpler than Celery)
- Fernet key generation automated in setup script
- Connection setup script supports both creation and update
- Quick start guide provides 5-minute path to working Airflow
- All DAG files already exist with placeholder implementations

## Next Steps
- DP-002: Implement daily price ingestion DAG (depends on DP-001)
- DB-004: Implement database functions (blocks DP-002)
- Test actual Airflow startup with: `./scripts/setup_airflow.sh docker`
